{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4","authorship_tag":"ABX9TyMIkhgUC2s7MdSqqAyT4S2H"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DEhQxLvjyw6M","executionInfo":{"status":"ok","timestamp":1759849939379,"user_tz":-360,"elapsed":7245176,"user":{"displayName":"Affshafee Rahman","userId":"16838251676656766360"}},"outputId":"da5cccdf-7e95-40cc-98ba-ffd5f621bd70"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","‚úÖ Libraries imported successfully\n","üîß PyTorch: 2.8.0+cu126, CUDA: True\n","\n","================================================================================\n","üì¶ EXTRACTING AVLIPS DATASET\n","================================================================================\n","‚è≥ Extracting...\n","‚úÖ Extracted in 235.1 seconds\n","‚úÖ Config loaded: cuda\n","üå± Seeds: [42, 123, 456, 789, 2024]\n","üìä Samples per seed: 4000 (2000 real + 2000 fake)\n","\n","================================================================================\n","üóÑÔ∏è  CACHING ENTIRE AVLIPS DATASET INTO RAM (THIS TAKES ~30-40 MIN)\n","================================================================================\n","Found: 3396 real, 4206 fake videos\n"]},{"output_type":"stream","name":"stderr","text":["Caching ALL REAL: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 54/54 [33:39<00:00, 37.39s/it]\n"]},{"output_type":"stream","name":"stdout","text":["‚úÖ Cached 1519 real videos\n"]},{"output_type":"stream","name":"stderr","text":["Caching ALL FAKE: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 66/66 [41:25<00:00, 37.66s/it]\n"]},{"output_type":"stream","name":"stdout","text":["‚úÖ Cached 2210 fake videos\n","\n","üéâ FULL DATASET CACHED! Real: 1519, Fake: 2210\n","\n","================================================================================\n","üå± TRAINING 5 MODELS WITH DIFFERENT SEEDS\n","================================================================================\n","\n","================================================================================\n","üå± SEED 1/5: 42\n","================================================================================\n","üìä Sampled: 1519 real + 2000 fake = 3519 total\n","Split: Train=2463, Val=528, Test=528\n","Downloading: \"https://download.pytorch.org/models/mobilenet_v3_small-047dcff4.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v3_small-047dcff4.pth\n"]},{"output_type":"stream","name":"stderr","text":["100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9.83M/9.83M [00:00<00:00, 137MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["üöÄ Training for 20 epochs...\n","\n","--- Epoch 1/20 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [05:56<00:00,  9.14s/it]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:06<00:00,  1.30it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.7008, Val Loss: 0.6869\n","üèÜ Best model saved!\n","\n","--- Epoch 2/20 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:33<00:00,  1.15it/s]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:04<00:00,  2.24it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.6676, Val Loss: 0.6753\n","üèÜ Best model saved!\n","\n","--- Epoch 3/20 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:34<00:00,  1.14it/s]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:03<00:00,  2.25it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.6160, Val Loss: 0.6832\n","\n","--- Epoch 4/20 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:33<00:00,  1.15it/s]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:04<00:00,  2.24it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.5162, Val Loss: 0.7700\n","\n","--- Epoch 5/20 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:34<00:00,  1.13it/s]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:04<00:00,  2.17it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.4400, Val Loss: 0.8669\n","\n","--- Epoch 6/20 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:34<00:00,  1.13it/s]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:04<00:00,  2.22it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.3831, Val Loss: 0.8489\n","\n","--- Epoch 7/20 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:33<00:00,  1.15it/s]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:04<00:00,  2.24it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.3456, Val Loss: 0.9316\n","üõë Early stopping\n"]},{"output_type":"stream","name":"stderr","text":["Testing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:52<00:00,  5.86s/it]\n"]},{"output_type":"stream","name":"stdout","text":["\n","‚úÖ SEED 42 RESULTS:\n","   Accuracy: 0.5758 (57.58%)\n","   AUC:      0.5827 (58.27%)\n","\n","================================================================================\n","üå± SEED 2/5: 123\n","================================================================================\n","üìä Sampled: 1519 real + 2000 fake = 3519 total\n","Split: Train=2463, Val=528, Test=528\n","üöÄ Training for 20 epochs...\n","\n","--- Epoch 1/20 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:35<00:00,  1.09it/s]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:04<00:00,  2.11it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.6970, Val Loss: 0.6842\n","üèÜ Best model saved!\n","\n","--- Epoch 2/20 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:35<00:00,  1.10it/s]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:04<00:00,  2.11it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.6752, Val Loss: 0.6737\n","üèÜ Best model saved!\n","\n","--- Epoch 3/20 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:35<00:00,  1.08it/s]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:04<00:00,  2.07it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.6309, Val Loss: 0.6339\n","üèÜ Best model saved!\n","\n","--- Epoch 4/20 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:36<00:00,  1.07it/s]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:04<00:00,  2.12it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.5396, Val Loss: 0.5930\n","üèÜ Best model saved!\n","\n","--- Epoch 5/20 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:35<00:00,  1.10it/s]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:04<00:00,  2.06it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.4415, Val Loss: 0.6784\n","\n","--- Epoch 6/20 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:35<00:00,  1.09it/s]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:04<00:00,  2.08it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.3819, Val Loss: 0.9185\n","\n","--- Epoch 7/20 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:35<00:00,  1.10it/s]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:04<00:00,  2.06it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.3530, Val Loss: 0.8967\n","\n","--- Epoch 8/20 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:35<00:00,  1.09it/s]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:04<00:00,  2.10it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.3197, Val Loss: 0.6868\n","\n","--- Epoch 9/20 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:36<00:00,  1.08it/s]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:04<00:00,  2.09it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.2971, Val Loss: 0.7159\n","üõë Early stopping\n"]},{"output_type":"stream","name":"stderr","text":["Testing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:04<00:00,  2.09it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","‚úÖ SEED 123 RESULTS:\n","   Accuracy: 0.7064 (70.64%)\n","   AUC:      0.8054 (80.54%)\n","\n","================================================================================\n","üå± SEED 3/5: 456\n","================================================================================\n","üìä Sampled: 1519 real + 2000 fake = 3519 total\n","Split: Train=2463, Val=528, Test=528\n","üöÄ Training for 20 epochs...\n","\n","--- Epoch 1/20 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:35<00:00,  1.09it/s]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:04<00:00,  2.10it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.6931, Val Loss: 0.6879\n","üèÜ Best model saved!\n","\n","--- Epoch 2/20 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:35<00:00,  1.10it/s]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:04<00:00,  2.11it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.6653, Val Loss: 0.6661\n","üèÜ Best model saved!\n","\n","--- Epoch 3/20 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:35<00:00,  1.10it/s]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:04<00:00,  2.10it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.6115, Val Loss: 0.6290\n","üèÜ Best model saved!\n","\n","--- Epoch 4/20 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:35<00:00,  1.09it/s]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:04<00:00,  2.10it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.5031, Val Loss: 0.7341\n","\n","--- Epoch 5/20 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:35<00:00,  1.09it/s]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:04<00:00,  2.10it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.4222, Val Loss: 0.7773\n","\n","--- Epoch 6/20 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:36<00:00,  1.07it/s]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:04<00:00,  2.07it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.3724, Val Loss: 0.8441\n","\n","--- Epoch 7/20 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:35<00:00,  1.09it/s]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:04<00:00,  2.09it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.3357, Val Loss: 0.8058\n","\n","--- Epoch 8/20 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:35<00:00,  1.09it/s]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:04<00:00,  2.06it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.3207, Val Loss: 0.6905\n","üõë Early stopping\n"]},{"output_type":"stream","name":"stderr","text":["Testing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:04<00:00,  2.09it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","‚úÖ SEED 456 RESULTS:\n","   Accuracy: 0.6667 (66.67%)\n","   AUC:      0.7382 (73.82%)\n","\n","================================================================================\n","üå± SEED 4/5: 789\n","================================================================================\n","üìä Sampled: 1519 real + 2000 fake = 3519 total\n","Split: Train=2463, Val=528, Test=528\n","üöÄ Training for 20 epochs...\n","\n","--- Epoch 1/20 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:35<00:00,  1.10it/s]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:04<00:00,  2.10it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.6956, Val Loss: 0.6863\n","üèÜ Best model saved!\n","\n","--- Epoch 2/20 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:35<00:00,  1.10it/s]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:04<00:00,  2.10it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.6684, Val Loss: 0.6856\n","üèÜ Best model saved!\n","\n","--- Epoch 3/20 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:35<00:00,  1.10it/s]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:04<00:00,  2.11it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.6053, Val Loss: 0.6459\n","üèÜ Best model saved!\n","\n","--- Epoch 4/20 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:35<00:00,  1.09it/s]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:04<00:00,  2.08it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.5091, Val Loss: 0.6459\n","\n","--- Epoch 5/20 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:35<00:00,  1.09it/s]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:04<00:00,  2.10it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.4297, Val Loss: 0.7994\n","\n","--- Epoch 6/20 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:36<00:00,  1.07it/s]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:04<00:00,  2.10it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.3617, Val Loss: 0.6655\n","\n","--- Epoch 7/20 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:35<00:00,  1.09it/s]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:04<00:00,  2.10it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.3376, Val Loss: 0.6124\n","üèÜ Best model saved!\n","\n","--- Epoch 8/20 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:35<00:00,  1.09it/s]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:04<00:00,  2.08it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.3047, Val Loss: 0.6341\n","\n","--- Epoch 9/20 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:36<00:00,  1.08it/s]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:04<00:00,  2.08it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.2876, Val Loss: 0.5460\n","üèÜ Best model saved!\n","\n","--- Epoch 10/20 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:35<00:00,  1.10it/s]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:04<00:00,  2.06it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.2699, Val Loss: 0.5110\n","üèÜ Best model saved!\n","\n","--- Epoch 11/20 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:35<00:00,  1.09it/s]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:04<00:00,  2.08it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.2574, Val Loss: 0.4539\n","üèÜ Best model saved!\n","\n","--- Epoch 12/20 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:35<00:00,  1.09it/s]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:04<00:00,  2.10it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.2539, Val Loss: 0.4478\n","üèÜ Best model saved!\n","\n","--- Epoch 13/20 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:35<00:00,  1.09it/s]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:04<00:00,  2.09it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.2450, Val Loss: 0.4184\n","üèÜ Best model saved!\n","\n","--- Epoch 14/20 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:35<00:00,  1.08it/s]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:04<00:00,  2.06it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.2381, Val Loss: 0.4213\n","\n","--- Epoch 15/20 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:36<00:00,  1.07it/s]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:04<00:00,  2.10it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.2358, Val Loss: 0.4311\n","\n","--- Epoch 16/20 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:35<00:00,  1.09it/s]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:04<00:00,  2.09it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.2303, Val Loss: 0.4601\n","\n","--- Epoch 17/20 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:35<00:00,  1.09it/s]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:04<00:00,  2.08it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.2254, Val Loss: 0.3616\n","üèÜ Best model saved!\n","\n","--- Epoch 18/20 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:35<00:00,  1.09it/s]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:04<00:00,  2.10it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.2243, Val Loss: 0.3177\n","üèÜ Best model saved!\n","\n","--- Epoch 19/20 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:35<00:00,  1.09it/s]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:04<00:00,  2.09it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.2182, Val Loss: 0.3250\n","\n","--- Epoch 20/20 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:35<00:00,  1.09it/s]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:04<00:00,  2.08it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.2186, Val Loss: 0.4054\n"]},{"output_type":"stream","name":"stderr","text":["Testing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:04<00:00,  2.06it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","‚úÖ SEED 789 RESULTS:\n","   Accuracy: 0.9318 (93.18%)\n","   AUC:      0.9820 (98.20%)\n","\n","================================================================================\n","üå± SEED 5/5: 2024\n","================================================================================\n","üìä Sampled: 1519 real + 2000 fake = 3519 total\n","Split: Train=2463, Val=528, Test=528\n","üöÄ Training for 20 epochs...\n","\n","--- Epoch 1/20 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:35<00:00,  1.10it/s]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:04<00:00,  2.10it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.6871, Val Loss: 0.6897\n","üèÜ Best model saved!\n","\n","--- Epoch 2/20 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:35<00:00,  1.10it/s]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:04<00:00,  2.05it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.6587, Val Loss: 0.6701\n","üèÜ Best model saved!\n","\n","--- Epoch 3/20 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:35<00:00,  1.09it/s]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:04<00:00,  2.09it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.5853, Val Loss: 0.6079\n","üèÜ Best model saved!\n","\n","--- Epoch 4/20 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:35<00:00,  1.09it/s]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:04<00:00,  2.08it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.4847, Val Loss: 0.7311\n","\n","--- Epoch 5/20 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:35<00:00,  1.09it/s]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:04<00:00,  2.10it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.4227, Val Loss: 0.8947\n","\n","--- Epoch 6/20 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:35<00:00,  1.09it/s]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:04<00:00,  2.09it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.3684, Val Loss: 0.8178\n","\n","--- Epoch 7/20 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:36<00:00,  1.08it/s]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:04<00:00,  2.09it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.3467, Val Loss: 0.7528\n","\n","--- Epoch 8/20 ---\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:35<00:00,  1.09it/s]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:04<00:00,  2.08it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.3196, Val Loss: 0.8325\n","üõë Early stopping\n"]},{"output_type":"stream","name":"stderr","text":["Testing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:04<00:00,  2.07it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","‚úÖ SEED 2024 RESULTS:\n","   Accuracy: 0.6496 (64.96%)\n","   AUC:      0.7183 (71.83%)\n","\n","================================================================================\n","üìä MULTI-SEED TRAINING COMPLETE!\n","================================================================================\n","\n"," seed  accuracy      auc\n","   42  0.575758 0.582675\n","  123  0.706439 0.805380\n","  456  0.666667 0.738202\n","  789  0.931818 0.982018\n"," 2024  0.649621 0.718333\n","\n","Accuracy: 70.61% ¬± 13.48%\n","AUC:      76.53% ¬± 14.57%\n","\n","‚úÖ Saved: /content/transformer_multiseed_results.csv\n","================================================================================\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["# -*- coding: utf-8 -*-\n","\n","\"\"\"\n","TRANSFORMER V1: MULTI-SEED TRAINING (5 SEEDS)\n","Caches entire AVLips dataset once, then trains 5 models with different seeds\n","Each seed samples 2000 real + 2000 fake from cached data\n","Optimized for Google Colab Pro (53GB RAM, 22.5GB VRAM)\n","\"\"\"\n","\n","# ============================================================================\n","# STEP 1: MOUNT DRIVE & IMPORTS\n","# ============================================================================\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os\n","import cv2\n","import time\n","import torch\n","import zipfile\n","import librosa\n","import numpy as np\n","import pandas as pd\n","import torch.nn as nn\n","from pathlib import Path\n","from tqdm import tqdm\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","import matplotlib.pyplot as plt\n","import torch.nn.functional as F\n","import torchvision.models as models\n","from torch.cuda.amp import autocast, GradScaler\n","import torchvision.transforms as transforms\n","from torch.utils.data import Dataset, DataLoader, Subset\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report, roc_auc_score\n","import warnings\n","\n","warnings.filterwarnings('ignore')\n","torch.backends.cudnn.benchmark = True\n","print(\"‚úÖ Libraries imported successfully\")\n","print(f\"üîß PyTorch: {torch.__version__}, CUDA: {torch.cuda.is_available()}\")\n","\n","# ============================================================================\n","# STEP 2: EXTRACT AVLIPS DATASET\n","# ============================================================================\n","\n","print(\"\\n\" + \"=\"*80)\n","print(\"üì¶ EXTRACTING AVLIPS DATASET\")\n","print(\"=\"*80)\n","\n","zip_path = \"/content/drive/MyDrive/CSE400 codes - 144/AVLips.zip\"\n","extract_path = '/content/AVLips_data'\n","\n","if not os.path.exists(extract_path):\n","    print(f\"‚è≥ Extracting...\")\n","    start_time = time.time()\n","    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n","        zip_ref.extractall(extract_path)\n","    print(f\"‚úÖ Extracted in {time.time()-start_time:.1f} seconds\")\n","else:\n","    print(f\"‚úÖ Already extracted\")\n","\n","# ============================================================================\n","# STEP 3: CONFIGURATION\n","# ============================================================================\n","\n","class Config:\n","    def __init__(self):\n","        self.data_dir = \"/content/AVLips_data/AVLips\"\n","        self.model_save_dir = \"/content/models_multiseed/\"\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        os.makedirs(self.model_save_dir, exist_ok=True)\n","\n","        # Visual Stream\n","        self.vis_image_size = (128, 128)\n","        self.vis_num_frames = 16\n","        self.vis_cnn_feature_dim = 576\n","        self.vis_transformer_d_model = 256\n","        self.vis_transformer_nhead = 8\n","        self.vis_transformer_layers = 4\n","        self.vis_transformer_dropout = 0.1\n","\n","        # Audio Stream\n","        self.aud_sample_rate = 16000\n","        self.aud_num_chunks = 5\n","        self.aud_chunk_duration = 1.0\n","        self.aud_n_mels = 128\n","        self.aud_cnn_feature_dim = 576\n","        self.aud_transformer_d_model = 256\n","        self.aud_transformer_nhead = 8\n","        self.aud_transformer_layers = 4\n","        self.aud_transformer_dropout = 0.1\n","\n","        # Training\n","        self.batch_size = 64\n","        self.accumulation_steps = 4\n","        self.epochs = 20  # Reduced for faster multi-seed training\n","        self.learning_rate = 5e-5\n","        self.weight_decay = 0.05\n","        self.patience = 5\n","        self.gradient_clip = 1.0\n","\n","        # Multi-seed config\n","        self.seeds = [42, 123, 456, 789, 2024]\n","        self.samples_per_class_per_seed = 2000  # 2000 real + 2000 fake per seed\n","\n","config = Config()\n","print(f\"‚úÖ Config loaded: {config.device}\")\n","print(f\"üå± Seeds: {config.seeds}\")\n","print(f\"üìä Samples per seed: {config.samples_per_class_per_seed * 2} (2000 real + 2000 fake)\")\n","\n","# ============================================================================\n","# STEP 4: DATA PROCESSING\n","# ============================================================================\n","\n","def process_visual_stream(video_path: str, config: Config):\n","    cap = cv2.VideoCapture(video_path)\n","    if not cap.isOpened():\n","        return None\n","\n","    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","    if total_frames < config.vis_num_frames:\n","        return None\n","\n","    frame_indices = np.linspace(0, total_frames - 1, config.vis_num_frames, dtype=int)\n","    frames = []\n","    face_detector = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n","\n","    for i in frame_indices:\n","        cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n","        ret, frame = cap.read()\n","        if not ret:\n","            continue\n","\n","        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n","        faces = face_detector.detectMultiScale(gray, 1.1, 4)\n","\n","        if len(faces) > 0:\n","            (x, y, w, h) = faces[0]\n","            mouth_crop = frame[y + int(h * 0.6):y + h, x + int(w * 0.25):x + int(w * 0.75)]\n","            if mouth_crop.size > 0:\n","                resized_crop = cv2.resize(mouth_crop, config.vis_image_size)\n","                resized_crop_rgb = cv2.cvtColor(resized_crop, cv2.COLOR_BGR2RGB)\n","                frames.append(resized_crop_rgb)\n","\n","    cap.release()\n","    return np.stack(frames) if len(frames) == config.vis_num_frames else None\n","\n","def process_audio_stream(video_path: str, config: Config):\n","    try:\n","        parts = Path(video_path).parts\n","        audio_filename = Path(video_path).stem + \".wav\"\n","        label_folder = parts[-2]\n","        base_data_dir = str(Path(video_path).parent.parent)\n","        audio_path = os.path.join(base_data_dir, \"wav\", label_folder, audio_filename)\n","\n","        y, sr = librosa.load(audio_path, sr=config.aud_sample_rate)\n","        total_samples = int(config.aud_chunk_duration * config.aud_num_chunks * sr)\n","\n","        if len(y) < total_samples:\n","            y = np.pad(y, (0, total_samples - len(y)), mode='constant')\n","        else:\n","            y = y[:total_samples]\n","\n","        samples_per_chunk = int(config.aud_chunk_duration * sr)\n","        mel_list = []\n","\n","        for i in range(config.aud_num_chunks):\n","            chunk = y[i*samples_per_chunk : (i+1)*samples_per_chunk]\n","            mel = librosa.feature.melspectrogram(y=chunk, sr=sr, n_mels=config.aud_n_mels)\n","            mel_db = librosa.power_to_db(mel, ref=np.max)\n","            mel_db = (mel_db - mel_db.mean()) / (mel_db.std() + 1e-9)\n","            mel_list.append(torch.tensor(mel_db, dtype=torch.float32))\n","\n","        return torch.stack(mel_list, axis=0)\n","    except Exception:\n","        return None\n","\n","# ============================================================================\n","# STEP 5: DATASET CLASSES\n","# ============================================================================\n","\n","class DualStreamDataset(Dataset):\n","    def __init__(self, file_paths, labels, config):\n","        self.file_paths = file_paths\n","        self.labels = labels\n","        self.config = config\n","\n","    def __len__(self):\n","        return len(self.file_paths)\n","\n","    def __getitem__(self, idx):\n","        try:\n","            visual_frames_hwc = process_visual_stream(self.file_paths[idx], self.config)\n","            if visual_frames_hwc is None:\n","                return None\n","\n","            visual_frames_tchw = visual_frames_hwc.transpose(0, 3, 1, 2)\n","            audio_mels = process_audio_stream(self.file_paths[idx], self.config)\n","            if audio_mels is None:\n","                return None\n","\n","            return (visual_frames_tchw, audio_mels.unsqueeze(1)), torch.tensor(self.labels[idx], dtype=torch.float32)\n","        except Exception:\n","            return None\n","\n","class RAMCachedDataset(Dataset):\n","    def __init__(self, data, labels, transform=None):\n","        self.data = data\n","        self.labels = labels\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def __getitem__(self, idx):\n","        visual_frames_np, audio_tensor = self.data[idx]\n","        label = self.labels[idx]\n","\n","        if self.transform:\n","            augmented_frames = []\n","            for frame_np in visual_frames_np:\n","                frame_hwc = frame_np.transpose(1, 2, 0)\n","                augmented_frames.append(self.transform(frame_hwc))\n","            visual_tensor = torch.stack(augmented_frames)\n","        else:\n","            visual_tensor = torch.from_numpy(visual_frames_np).float()\n","\n","        return (visual_tensor, audio_tensor), label\n","\n","# ============================================================================\n","# STEP 6: TRANSFORMER MODEL\n","# ============================================================================\n","\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model: int, max_len: int = 5000):\n","        super().__init__()\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        batch_size, seq_len, _ = x.shape\n","        return x + self.pe[:seq_len, :].unsqueeze(0).expand(batch_size, -1, -1)\n","\n","class VisualStream_Transformer(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        mobilenet = models.mobilenet_v3_small(weights=models.MobileNet_V3_Small_Weights.DEFAULT)\n","        self.cnn_features = mobilenet.features\n","        self.avgpool = mobilenet.avgpool\n","        self.proj = nn.Linear(config.vis_cnn_feature_dim, config.vis_transformer_d_model)\n","        self.proj_dropout = nn.Dropout(0.3)\n","        self.pos_encoding = PositionalEncoding(config.vis_transformer_d_model, max_len=config.vis_num_frames)\n","\n","        encoder_layer = nn.TransformerEncoderLayer(\n","            d_model=config.vis_transformer_d_model, nhead=config.vis_transformer_nhead,\n","            dim_feedforward=config.vis_transformer_d_model * 4, dropout=config.vis_transformer_dropout,\n","            batch_first=True, norm_first=True\n","        )\n","        self.transformer = nn.TransformerEncoder(encoder_layer, config.vis_transformer_layers)\n","        self.layer_norm = nn.LayerNorm(config.vis_transformer_d_model)\n","        self.out_dim = config.vis_transformer_d_model\n","\n","    def forward(self, x):\n","        b, t, c, h, w = x.shape\n","        x = x.view(b * t, c, h, w)\n","        features = self.avgpool(self.cnn_features(x)).view(b, t, -1)\n","        projected = self.proj_dropout(self.proj(features))\n","        encoded = self.pos_encoding(projected)\n","        transformer_out = self.transformer(encoded)\n","        return self.layer_norm(transformer_out[:, -1, :])\n","\n","class AudioStream_Transformer(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        mobilenet = models.mobilenet_v3_small(weights=models.MobileNet_V3_Small_Weights.DEFAULT)\n","        self.cnn_features = mobilenet.features\n","        self.avgpool = mobilenet.avgpool\n","        self.proj = nn.Linear(config.aud_cnn_feature_dim, config.aud_transformer_d_model)\n","        self.proj_dropout = nn.Dropout(0.3)\n","        self.pos_encoding = PositionalEncoding(config.aud_transformer_d_model, max_len=config.aud_num_chunks)\n","\n","        encoder_layer = nn.TransformerEncoderLayer(\n","            d_model=config.aud_transformer_d_model, nhead=config.aud_transformer_nhead,\n","            dim_feedforward=config.aud_transformer_d_model * 4, dropout=config.aud_transformer_dropout,\n","            batch_first=True, norm_first=True\n","        )\n","        self.transformer = nn.TransformerEncoder(encoder_layer, config.aud_transformer_layers)\n","        self.layer_norm = nn.LayerNorm(config.aud_transformer_d_model)\n","        self.out_dim = config.aud_transformer_d_model\n","\n","    def forward(self, x):\n","        b, t, c, h, w = x.shape\n","        x = x.view(b * t, c, h, w).repeat(1, 3, 1, 1)\n","        features = self.avgpool(self.cnn_features(x)).view(b, t, -1)\n","        projected = self.proj_dropout(self.proj(features))\n","        encoded = self.pos_encoding(projected)\n","        transformer_out = self.transformer(encoded)\n","        return self.layer_norm(transformer_out[:, -1, :])\n","\n","class FusionModel_Transformer(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.visual_stream = VisualStream_Transformer(config)\n","        self.audio_stream = AudioStream_Transformer(config)\n","        fusion_input_dim = self.visual_stream.out_dim + self.audio_stream.out_dim\n","        self.fusion_head = nn.Sequential(\n","            nn.Linear(fusion_input_dim, 256), nn.ReLU(), nn.Dropout(0.6), nn.Linear(256, 1)\n","        )\n","\n","    def forward(self, visual_input, audio_input):\n","        visual_features = self.visual_stream(visual_input)\n","        audio_features = self.audio_stream(audio_input)\n","        return self.fusion_head(torch.cat((visual_features, audio_features), dim=1))\n","\n","# ============================================================================\n","# STEP 7: TRAINING FUNCTIONS\n","# ============================================================================\n","\n","class LabelSmoothingBCELoss(nn.Module):\n","    def __init__(self, smoothing=0.1):\n","        super().__init__()\n","        self.smoothing = smoothing\n","\n","    def forward(self, pred, target):\n","        target = target * (1 - self.smoothing) + 0.5 * self.smoothing\n","        return F.binary_cross_entropy_with_logits(pred, target)\n","\n","def train_one_epoch(model, loader, optimizer, criterion, scaler, config):\n","    model.train()\n","    total_loss = 0\n","    for i, ((visual_data, audio_data), labels) in enumerate(tqdm(loader, desc=\"Training\")):\n","        visual_data = visual_data.to(config.device, non_blocking=True)\n","        audio_data = audio_data.to(config.device, non_blocking=True)\n","        labels = labels.to(config.device, non_blocking=True).unsqueeze(1).float()\n","\n","        with autocast():\n","            outputs = model(visual_data, audio_data)\n","            loss = criterion(outputs, labels) / config.accumulation_steps\n","\n","        scaler.scale(loss).backward()\n","\n","        if (i + 1) % config.accumulation_steps == 0:\n","            scaler.unscale_(optimizer)\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=config.gradient_clip)\n","            scaler.step(optimizer)\n","            scaler.update()\n","            optimizer.zero_grad(set_to_none=True)\n","\n","        total_loss += loss.item() * config.accumulation_steps\n","\n","    return total_loss / len(loader)\n","\n","def validate_one_epoch(model, loader, criterion, config):\n","    model.eval()\n","    total_loss = 0\n","    with torch.no_grad():\n","        for (visual_data, audio_data), labels in tqdm(loader, desc=\"Validating\"):\n","            visual_data = visual_data.to(config.device, non_blocking=True)\n","            audio_data = audio_data.to(config.device, non_blocking=True)\n","            labels = labels.to(config.device, non_blocking=True).unsqueeze(1).float()\n","\n","            with autocast():\n","                loss = criterion(model(visual_data, audio_data), labels)\n","\n","            total_loss += loss.item()\n","\n","    return total_loss / len(loader)\n","\n","# ============================================================================\n","# STEP 8: CACHE ENTIRE AVLIPS DATASET (ONCE!)\n","# ============================================================================\n","\n","print(\"\\n\" + \"=\"*80)\n","print(\"üóÑÔ∏è  CACHING ENTIRE AVLIPS DATASET INTO RAM (THIS TAKES ~30-40 MIN)\")\n","print(\"=\"*80)\n","\n","real_dir = os.path.join(config.data_dir, \"0_real\")\n","fake_dir = os.path.join(config.data_dir, \"1_fake\")\n","\n","all_real_files = [os.path.join(real_dir, f) for f in os.listdir(real_dir) if f.endswith('.mp4')]\n","all_fake_files = [os.path.join(fake_dir, f) for f in os.listdir(fake_dir) if f.endswith('.mp4')]\n","\n","print(f\"Found: {len(all_real_files)} real, {len(all_fake_files)} fake videos\")\n","\n","def collate_fn_skip_errors(batch):\n","    batch = list(filter(lambda x: x is not None, batch))\n","    return torch.utils.data.dataloader.default_collate(batch) if batch else (None, None)\n","\n","def cache_all_data(files, labels, desc):\n","    dataset = DualStreamDataset(files, labels, config)\n","    loader = DataLoader(dataset, batch_size=config.batch_size, num_workers=os.cpu_count(), collate_fn=collate_fn_skip_errors)\n","    cached_data, cached_labels = [], []\n","\n","    for data, batch_labels in tqdm(loader, desc=f\"Caching {desc}\"):\n","        if data is not None:\n","            visual_batch, audio_batch = data\n","            for i in range(visual_batch.shape[0]):\n","                cached_data.append((visual_batch[i].numpy(), audio_batch[i]))\n","                cached_labels.append(batch_labels[i].item())\n","\n","    return cached_data, cached_labels\n","\n","# Cache ALL real videos\n","all_real_cached, all_real_labels = cache_all_data(all_real_files, [0]*len(all_real_files), \"ALL REAL\")\n","print(f\"‚úÖ Cached {len(all_real_cached)} real videos\")\n","\n","# Cache ALL fake videos\n","all_fake_cached, all_fake_labels = cache_all_data(all_fake_files, [1]*len(all_fake_files), \"ALL FAKE\")\n","print(f\"‚úÖ Cached {len(all_fake_cached)} fake videos\")\n","\n","print(f\"\\nüéâ FULL DATASET CACHED! Real: {len(all_real_cached)}, Fake: {len(all_fake_cached)}\")\n","\n","# ============================================================================\n","# STEP 9: MULTI-SEED TRAINING LOOP\n","# ============================================================================\n","\n","print(\"\\n\" + \"=\"*80)\n","print(f\"üå± TRAINING {len(config.seeds)} MODELS WITH DIFFERENT SEEDS\")\n","print(\"=\"*80)\n","\n","train_transform = transforms.Compose([\n","    transforms.ToPILImage(),\n","    transforms.RandomHorizontalFlip(p=0.5),\n","    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","val_test_transform = transforms.Compose([\n","    transforms.ToPILImage(),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","all_results = []\n","\n","for seed_idx, seed in enumerate(config.seeds):\n","    print(f\"\\n{'='*80}\")\n","    print(f\"üå± SEED {seed_idx+1}/{len(config.seeds)}: {seed}\")\n","    print(f\"{'='*80}\")\n","\n","    # Set seed for reproducibility\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed(seed)\n","\n","    # Sample 2000 real + 2000 fake from cached data\n","    real_indices = np.random.choice(len(all_real_cached), min(config.samples_per_class_per_seed, len(all_real_cached)), replace=False)\n","    fake_indices = np.random.choice(len(all_fake_cached), min(config.samples_per_class_per_seed, len(all_fake_cached)), replace=False)\n","\n","    sampled_real_data = [all_real_cached[i] for i in real_indices]\n","    sampled_fake_data = [all_fake_cached[i] for i in fake_indices]\n","    sampled_real_labels = [0] * len(sampled_real_data)\n","    sampled_fake_labels = [1] * len(sampled_fake_data)\n","\n","    # Combine\n","    all_sampled_data = sampled_real_data + sampled_fake_data\n","    all_sampled_labels = sampled_real_labels + sampled_fake_labels\n","\n","    print(f\"üìä Sampled: {len(sampled_real_data)} real + {len(sampled_fake_data)} fake = {len(all_sampled_data)} total\")\n","\n","    # Train/val/test split\n","    train_indices, temp_indices = train_test_split(range(len(all_sampled_data)), test_size=0.3, random_state=seed,\n","                                                    stratify=all_sampled_labels)\n","    val_indices, test_indices = train_test_split(temp_indices, test_size=0.5, random_state=seed,\n","                                                  stratify=[all_sampled_labels[i] for i in temp_indices])\n","\n","    train_data = [all_sampled_data[i] for i in train_indices]\n","    train_labels = torch.tensor([all_sampled_labels[i] for i in train_indices])\n","    val_data = [all_sampled_data[i] for i in val_indices]\n","    val_labels = torch.tensor([all_sampled_labels[i] for i in val_indices])\n","    test_data = [all_sampled_data[i] for i in test_indices]\n","    test_labels = torch.tensor([all_sampled_labels[i] for i in test_indices])\n","\n","    print(f\"Split: Train={len(train_data)}, Val={len(val_data)}, Test={len(test_data)}\")\n","\n","    # Create datasets\n","    train_dataset = RAMCachedDataset(train_data, train_labels, transform=train_transform)\n","    val_dataset = RAMCachedDataset(val_data, val_labels, transform=val_test_transform)\n","    test_dataset = RAMCachedDataset(test_data, test_labels, transform=val_test_transform)\n","\n","    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=2, pin_memory=True)\n","    val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False, num_workers=2, pin_memory=True)\n","    test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False, num_workers=2, pin_memory=True)\n","\n","    # Build model\n","    model = FusionModel_Transformer(config).to(config.device)\n","    optimizer = optim.AdamW(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n","    criterion = LabelSmoothingBCELoss(smoothing=0.1)\n","    scaler = GradScaler()\n","    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, min_lr=1e-6)\n","\n","    model_path = os.path.join(config.model_save_dir, f'transformer_seed{seed}_best.pth')\n","    best_val_loss = float('inf')\n","    epochs_no_improve = 0\n","\n","    # Training loop\n","    print(f\"üöÄ Training for {config.epochs} epochs...\")\n","    for epoch in range(config.epochs):\n","        print(f\"\\n--- Epoch {epoch+1}/{config.epochs} ---\")\n","\n","        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, scaler, config)\n","        val_loss = validate_one_epoch(model, val_loader, criterion, config)\n","\n","        print(f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n","\n","        scheduler.step(val_loss)\n","\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            epochs_no_improve = 0\n","            torch.save(model.state_dict(), model_path)\n","            print(f\"üèÜ Best model saved!\")\n","        else:\n","            epochs_no_improve += 1\n","            if epochs_no_improve >= config.patience:\n","                print(f\"üõë Early stopping\")\n","                break\n","\n","    # Evaluate\n","    model.load_state_dict(torch.load(model_path))\n","    model.eval()\n","\n","    all_labels, all_preds = [], []\n","    with torch.no_grad():\n","        for (visual_data, audio_data), labels in tqdm(test_loader, desc=\"Testing\"):\n","            visual_data = visual_data.to(config.device)\n","            audio_data = audio_data.to(config.device)\n","            outputs = model(visual_data, audio_data)\n","            all_preds.extend(torch.sigmoid(outputs).cpu().numpy())\n","            all_labels.extend(labels.numpy())\n","\n","    all_preds = np.array(all_preds).flatten()\n","    all_labels = np.array(all_labels).flatten()\n","    preds_binary = (all_preds > 0.5).astype(int)\n","\n","    accuracy = (preds_binary == all_labels).mean()\n","    auc_score = roc_auc_score(all_labels, all_preds)\n","\n","    print(f\"\\n‚úÖ SEED {seed} RESULTS:\")\n","    print(f\"   Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n","    print(f\"   AUC:      {auc_score:.4f} ({auc_score*100:.2f}%)\")\n","\n","    all_results.append({'seed': seed, 'accuracy': accuracy, 'auc': auc_score})\n","\n","# ============================================================================\n","# FINAL SUMMARY\n","# ============================================================================\n","\n","print(\"\\n\" + \"=\"*80)\n","print(\"üìä MULTI-SEED TRAINING COMPLETE!\")\n","print(\"=\"*80)\n","\n","df = pd.DataFrame(all_results)\n","print(\"\\n\" + df.to_string(index=False))\n","\n","print(f\"\\nAccuracy: {df['accuracy'].mean()*100:.2f}% ¬± {df['accuracy'].std()*100:.2f}%\")\n","print(f\"AUC:      {df['auc'].mean()*100:.2f}% ¬± {df['auc'].std()*100:.2f}%\")\n","\n","df.to_csv('/content/transformer_multiseed_results.csv', index=False)\n","print(\"\\n‚úÖ Saved: /content/transformer_multiseed_results.csv\")\n","print(\"=\"*80)\n"]}]}