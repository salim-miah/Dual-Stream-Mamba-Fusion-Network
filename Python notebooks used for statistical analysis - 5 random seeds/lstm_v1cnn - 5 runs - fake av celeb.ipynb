{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4","mount_file_id":"1ZesDoEPIjZV25SnJeFZ5Pr4yZK2oB6Hm","authorship_tag":"ABX9TyO8aGsvJwZngSHcUOm7Ps5z"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["# -*- coding: utf-8 -*-\n","\n","\"\"\"\n","LSTM V1 CROSS-DATASET EVALUATION: FakeAVCeleb (WITH FACE DETECTION)\n","Uses SAME preprocessing as training (face detection + mouth crop)\n","\"\"\"\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os\n","import cv2\n","import time\n","import torch\n","import zipfile\n","import librosa\n","import numpy as np\n","import pandas as pd\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from pathlib import Path\n","from tqdm import tqdm\n","import torchvision.models as models\n","from torch.cuda.amp import autocast\n","import torchvision.transforms as transforms\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, accuracy_score\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import warnings\n","\n","warnings.filterwarnings('ignore')\n","torch.backends.cudnn.benchmark = True\n","print(\"âœ… Libraries imported\")\n","\n","# ============================================================================\n","# EXTRACT DATASET\n","# ============================================================================\n","\n","print(\"\\n\" + \"=\"*80)\n","print(\"ðŸ“¦ EXTRACTING FAKEAVCELEB\")\n","print(\"=\"*80)\n","\n","zip_path = '/content/drive/MyDrive/CSE400 codes - 144/archive.zip'\n","extract_path = '/content/FakeAVCeleb'\n","\n","if not os.path.exists(extract_path):\n","    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n","        zip_ref.extractall(extract_path)\n","    print(\"âœ… Extracted\")\n","else:\n","    print(\"âœ… Already extracted\")\n","\n","# ============================================================================\n","# CONFIGURATION\n","# ============================================================================\n","\n","class Config:\n","    def __init__(self):\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        self.vis_image_size = (128, 128)\n","        self.vis_num_frames = 16\n","        self.vis_cnn_feature_dim = 576\n","        self.vis_lstm_hidden = 128\n","        self.aud_sample_rate = 16000\n","        self.aud_num_chunks = 5\n","        self.aud_chunk_duration = 1.0\n","        self.aud_n_mels = 128\n","        self.aud_cnn_feature_dim = 576\n","        self.aud_lstm_hidden = 128\n","        self.lstm_num_layers = 2\n","        self.lstm_bidirectional = True\n","        self.lstm_dropout = 0.2\n","        self.batch_size = 64  # Reduced for stability\n","        self.num_workers = 2\n","\n","config = Config()\n","print(f\"âœ… Config loaded\")\n","\n","# ============================================================================\n","# DATA PROCESSING WITH FACE DETECTION (MATCHES TRAINING)\n","# ============================================================================\n","\n","def process_visual_with_face_detection(video_path: str, config: Config):\n","    \"\"\"EXACT MATCH to training: face detection + mouth crop\"\"\"\n","    cap = cv2.VideoCapture(video_path)\n","    if not cap.isOpened():\n","        return None\n","\n","    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","    if total_frames < config.vis_num_frames:\n","        cap.release()\n","        return None\n","\n","    frame_indices = np.linspace(0, total_frames - 1, config.vis_num_frames, dtype=int)\n","    frames = []\n","    face_detector = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n","\n","    for i in frame_indices:\n","        cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n","        ret, frame = cap.read()\n","        if not ret:\n","            continue\n","\n","        # Face detection\n","        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n","        faces = face_detector.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=4)\n","\n","        if len(faces) > 0:\n","            # Get largest face\n","            (x, y, w, h) = max(faces, key=lambda rect: rect[2] * rect[3])\n","\n","            # Mouth region crop (EXACT MATCH TO TRAINING)\n","            mouth_y_start = y + int(h * 0.6)\n","            mouth_y_end = y + h\n","            mouth_x_start = x + int(w * 0.25)\n","            mouth_x_end = x + int(w * 0.75)\n","\n","            mouth_crop = frame[mouth_y_start:mouth_y_end, mouth_x_start:mouth_x_end]\n","\n","            if mouth_crop.size > 0:\n","                resized_crop = cv2.resize(mouth_crop, config.vis_image_size)\n","                resized_crop_rgb = cv2.cvtColor(resized_crop, cv2.COLOR_BGR2RGB)\n","                frames.append(resized_crop_rgb)\n","            else:\n","                # Fallback: center crop\n","                h, w = frame.shape[:2]\n","                crop_size = min(h, w)\n","                start_h = (h - crop_size) // 2\n","                start_w = (w - crop_size) // 2\n","                cropped = frame[start_h:start_h+crop_size, start_w:start_w+crop_size]\n","                resized = cv2.resize(cropped, config.vis_image_size)\n","                resized_rgb = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n","                frames.append(resized_rgb)\n","        else:\n","            # Fallback: center crop\n","            h, w = frame.shape[:2]\n","            crop_size = min(h, w)\n","            start_h = (h - crop_size) // 2\n","            start_w = (w - crop_size) // 2\n","            cropped = frame[start_h:start_h+crop_size, start_w:start_w+crop_size]\n","            resized = cv2.resize(cropped, config.vis_image_size)\n","            resized_rgb = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n","            frames.append(resized_rgb)\n","\n","    cap.release()\n","    return np.stack(frames) if len(frames) == config.vis_num_frames else None\n","\n","def process_audio_from_video(video_path: str, config: Config):\n","    \"\"\"Extract audio from video\"\"\"\n","    try:\n","        y, sr = librosa.load(video_path, sr=config.aud_sample_rate, duration=5.0, mono=True)\n","        target_length = int(config.aud_sample_rate * config.aud_chunk_duration * config.aud_num_chunks)\n","\n","        if len(y) < target_length:\n","            y = np.pad(y, (0, target_length - len(y)), mode='constant')\n","        else:\n","            y = y[:target_length]\n","\n","        samples_per_chunk = int(config.aud_chunk_duration * sr)\n","        mel_spectrograms = []\n","\n","        for i in range(config.aud_num_chunks):\n","            chunk = y[i*samples_per_chunk : (i+1)*samples_per_chunk]\n","            mel = librosa.feature.melspectrogram(y=chunk, sr=sr, n_mels=config.aud_n_mels)\n","            mel_db = librosa.power_to_db(mel, ref=np.max)\n","            mel_db = (mel_db - mel_db.mean()) / (mel_db.std() + 1e-9)\n","            mel_spectrograms.append(torch.tensor(mel_db, dtype=torch.float32))\n","\n","        return torch.stack(mel_spectrograms, dim=0)\n","    except Exception:\n","        return None\n","\n","# ============================================================================\n","# DATASET\n","# ============================================================================\n","\n","class FakeAVCelebDataset(Dataset):\n","    def __init__(self, video_paths, labels, config):\n","        self.video_paths = video_paths\n","        self.labels = labels\n","        self.config = config\n","        self.transform = transforms.Compose([\n","            transforms.ToPILImage(),\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","        ])\n","\n","    def __len__(self):\n","        return len(self.video_paths)\n","\n","    def __getitem__(self, idx):\n","        try:\n","            visual_frames = process_visual_with_face_detection(self.video_paths[idx], self.config)\n","            if visual_frames is None:\n","                return None\n","\n","            visual_tensors = [self.transform(frame) for frame in visual_frames]\n","            visual_tensor = torch.stack(visual_tensors, dim=0)\n","\n","            audio_mels = process_audio_from_video(self.video_paths[idx], self.config)\n","            if audio_mels is None:\n","                return None\n","\n","            return visual_tensor, audio_mels.unsqueeze(1), torch.tensor(self.labels[idx], dtype=torch.float32)\n","        except Exception:\n","            return None\n","\n","def collate_fn(batch):\n","    batch = [x for x in batch if x is not None]\n","    return torch.utils.data.dataloader.default_collate(batch) if batch else None\n","\n","# ============================================================================\n","# MODEL\n","# ============================================================================\n","\n","class VisualStream_LSTM(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        mobilenet = models.mobilenet_v3_small(weights=models.MobileNet_V3_Small_Weights.DEFAULT)\n","        self.cnn_features = mobilenet.features\n","        self.avgpool = mobilenet.avgpool\n","        self.proj = nn.Linear(config.vis_cnn_feature_dim, config.vis_lstm_hidden)\n","        self.proj_dropout = nn.Dropout(0.3)\n","        self.lstm = nn.LSTM(config.vis_lstm_hidden, config.vis_lstm_hidden, config.lstm_num_layers,\n","                           batch_first=True, bidirectional=config.lstm_bidirectional,\n","                           dropout=config.lstm_dropout if config.lstm_num_layers > 1 else 0.0)\n","        self.lstm_dropout = nn.Dropout(0.2)\n","        self.out_dim = config.vis_lstm_hidden * (2 if config.lstm_bidirectional else 1)\n","\n","    def forward(self, x):\n","        b, t, c, h, w = x.shape\n","        x = x.view(b * t, c, h, w)\n","        features = self.avgpool(self.cnn_features(x)).view(b, t, -1)\n","        projected = self.proj_dropout(self.proj(features))\n","        lstm_out, _ = self.lstm(projected)\n","        return self.lstm_dropout(lstm_out[:, -1, :])\n","\n","class AudioStream_LSTM(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        mobilenet = models.mobilenet_v3_small(weights=models.MobileNet_V3_Small_Weights.DEFAULT)\n","        self.cnn_features = mobilenet.features\n","        self.avgpool = mobilenet.avgpool\n","        self.proj = nn.Linear(config.aud_cnn_feature_dim, config.aud_lstm_hidden)\n","        self.proj_dropout = nn.Dropout(0.3)\n","        self.lstm = nn.LSTM(config.aud_lstm_hidden, config.aud_lstm_hidden, config.lstm_num_layers,\n","                           batch_first=True, bidirectional=config.lstm_bidirectional,\n","                           dropout=config.lstm_dropout if config.lstm_num_layers > 1 else 0.0)\n","        self.lstm_dropout = nn.Dropout(0.2)\n","        self.out_dim = config.aud_lstm_hidden * (2 if config.lstm_bidirectional else 1)\n","\n","    def forward(self, x):\n","        b, t, c, h, w = x.shape\n","        x = x.view(b * t, c, h, w).repeat(1, 3, 1, 1)\n","        features = self.avgpool(self.cnn_features(x)).view(b, t, -1)\n","        projected = self.proj_dropout(self.proj(features))\n","        lstm_out, _ = self.lstm(projected)\n","        return self.lstm_dropout(lstm_out[:, -1, :])\n","\n","class FusionModel_LSTM(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.visual_stream = VisualStream_LSTM(config)\n","        self.audio_stream = AudioStream_LSTM(config)\n","        self.fusion_head = nn.Sequential(\n","            nn.Linear(self.visual_stream.out_dim + self.audio_stream.out_dim, 256),\n","            nn.ReLU(), nn.Dropout(0.6), nn.Linear(256, 1)\n","        )\n","\n","    def forward(self, visual, audio):\n","        return self.fusion_head(torch.cat([self.visual_stream(visual), self.audio_stream(audio)], dim=1))\n","\n","# ============================================================================\n","# FIND VIDEOS\n","# ============================================================================\n","\n","print(\"\\n\" + \"=\"*80)\n","print(\"ðŸ“‚ LOCATING VIDEOS\")\n","print(\"=\"*80)\n","\n","def find_videos(root_dir):\n","    videos = []\n","    for root, dirs, files in os.walk(root_dir):\n","        for file in files:\n","            if file.lower().endswith(('.mp4', '.avi', '.mov')):\n","                videos.append(os.path.join(root, file))\n","    return videos\n","\n","all_videos = find_videos(extract_path)\n","real_videos = [v for v in all_videos if 'RealVideo-RealAudio' in v]\n","fake_videos = [v for v in all_videos if 'FakeVideo' in v or ('fake' in v.lower() and 'RealVideo' not in v)]\n","print(f\"Real: {len(real_videos)}, Fake: {len(fake_videos)}\")\n","\n","# ============================================================================\n","# LOAD MODEL\n","# ============================================================================\n","\n","print(\"\\n\" + \"=\"*80)\n","print(\"ðŸ”§ LOADING MODEL\")\n","print(\"=\"*80)\n","\n","model_path = '/content/drive/MyDrive/PTHs/lstm_v1_best.pth'\n","model = FusionModel_LSTM(config).to(config.device)\n","model.load_state_dict(torch.load(model_path, map_location=config.device))\n","model.eval()\n","\n","for m in model.modules():\n","    if isinstance(m, nn.Dropout):\n","        m.p = 0.0\n","\n","print(f\"âœ… Model loaded\")\n","\n","# ============================================================================\n","# EVALUATION\n","# ============================================================================\n","\n","def evaluate_seed(seed, real_vids, fake_vids, model, config):\n","    print(f\"\\n{'='*80}\\nðŸŽ² SEED: {seed}\\n{'='*80}\")\n","    np.random.seed(seed)\n","\n","    real_sample = np.random.choice(real_vids, min(500, len(real_vids)), replace=False).tolist()\n","    fake_sample = np.random.choice(fake_vids, min(3000, len(fake_vids)), replace=False).tolist()\n","    test_videos = real_sample + fake_sample\n","    test_labels = [0]*len(real_sample) + [1]*len(fake_sample)\n","\n","    print(f\"Samples: {len(real_sample)} real + {len(fake_sample)} fake\")\n","\n","    dataset = FakeAVCelebDataset(test_videos, test_labels, config)\n","    loader = DataLoader(dataset, batch_size=config.batch_size, shuffle=False,\n","                       num_workers=config.num_workers, collate_fn=collate_fn, pin_memory=True)\n","\n","    all_preds, all_labels = [], []\n","\n","    with torch.no_grad():\n","        for batch in tqdm(loader, desc=f\"Seed {seed}\"):\n","            if batch is None:\n","                continue\n","            visual, audio, labels = batch\n","            with autocast():\n","                outputs = torch.sigmoid(model(visual.to(config.device), audio.to(config.device)))\n","            all_preds.extend(outputs.cpu().numpy())\n","            all_labels.extend(labels.numpy())\n","\n","    all_preds = np.array(all_preds).flatten()\n","    all_labels = np.array(all_labels).flatten()\n","\n","    valid_mask = ~np.isnan(all_preds)\n","    all_preds = all_preds[valid_mask]\n","    all_labels = all_labels[valid_mask]\n","\n","    if len(all_preds) < 10:\n","        return None\n","\n","    pred_binary = (all_preds > 0.5).astype(int)\n","    acc = accuracy_score(all_labels, pred_binary)\n","    auc = roc_auc_score(all_labels, all_preds)\n","    cm = confusion_matrix(all_labels, pred_binary)\n","    tn, fp, fn, tp = cm.ravel()\n","    prec = tp / (tp + fp) if (tp + fp) > 0 else 0\n","    rec = tp / (tp + fn) if (tp + fn) > 0 else 0\n","    f1 = 2 * prec * rec / (prec + rec) if (prec + rec) > 0 else 0\n","\n","    print(f\"\\nâœ… Results: Acc={acc:.4f}, AUC={auc:.4f}, F1={f1:.4f}\")\n","    return {'seed': seed, 'accuracy': acc, 'auc': auc, 'precision': prec, 'recall': rec, 'f1_score': f1}\n","\n","# ============================================================================\n","# RUN 5 SEEDS\n","# ============================================================================\n","\n","print(\"\\n\" + \"=\"*80)\n","print(\"ðŸš€ RUNNING 5 SEEDS\")\n","print(\"=\"*80)\n","\n","seeds = [42, 123, 456, 789, 2024]\n","results = []\n","\n","for seed in seeds:\n","    result = evaluate_seed(seed, real_videos, fake_videos, model, config)\n","    if result:\n","        results.append(result)\n","\n","if results:\n","    df = pd.DataFrame(results)\n","    print(\"\\n\" + \"=\"*80)\n","    print(\"ðŸ“Š FINAL STATISTICS\")\n","    print(\"=\"*80)\n","    for metric in ['accuracy', 'auc', 'precision', 'recall', 'f1_score']:\n","        print(f\"{metric.upper():12s}: {df[metric].mean()*100:5.2f}% Â± {df[metric].std()*100:4.2f}%\")\n","\n","    df.to_csv('/content/lstm_fakeavceleb_5runs.csv', index=False)\n","    print(\"\\nâœ… Saved: /content/lstm_fakeavceleb_5runs.csv\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ka9voRfR1JiH","executionInfo":{"status":"ok","timestamp":1759805202468,"user_tz":-360,"elapsed":5099191,"user":{"displayName":"Affshafee Rahman","userId":"16838251676656766360"}},"outputId":"78d6264f-e4ef-4bfc-c5bd-75eb53c0be08"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","âœ… Libraries imported\n","\n","================================================================================\n","ðŸ“¦ EXTRACTING FAKEAVCELEB\n","================================================================================\n","âœ… Extracted\n","âœ… Config loaded\n","\n","================================================================================\n","ðŸ“‚ LOCATING VIDEOS\n","================================================================================\n","Real: 500, Fake: 20544\n","\n","================================================================================\n","ðŸ”§ LOADING MODEL\n","================================================================================\n","Downloading: \"https://download.pytorch.org/models/mobilenet_v3_small-047dcff4.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v3_small-047dcff4.pth\n"]},{"output_type":"stream","name":"stderr","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9.83M/9.83M [00:00<00:00, 117MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["âœ… Model loaded\n","\n","================================================================================\n","ðŸš€ RUNNING 5 SEEDS\n","================================================================================\n","\n","================================================================================\n","ðŸŽ² SEED: 42\n","================================================================================\n","Samples: 500 real + 3000 fake\n"]},{"output_type":"stream","name":"stderr","text":["Seed 42: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 55/55 [16:43<00:00, 18.25s/it]\n"]},{"output_type":"stream","name":"stdout","text":["\n","âœ… Results: Acc=0.8471, AUC=0.8421, F1=0.9066\n","\n","================================================================================\n","ðŸŽ² SEED: 123\n","================================================================================\n","Samples: 500 real + 3000 fake\n"]},{"output_type":"stream","name":"stderr","text":["Seed 123: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 55/55 [16:25<00:00, 17.92s/it]\n"]},{"output_type":"stream","name":"stdout","text":["\n","âœ… Results: Acc=0.8469, AUC=0.8406, F1=0.9064\n","\n","================================================================================\n","ðŸŽ² SEED: 456\n","================================================================================\n","Samples: 500 real + 3000 fake\n"]},{"output_type":"stream","name":"stderr","text":["Seed 456: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 55/55 [16:24<00:00, 17.89s/it]\n"]},{"output_type":"stream","name":"stdout","text":["\n","âœ… Results: Acc=0.8477, AUC=0.8425, F1=0.9070\n","\n","================================================================================\n","ðŸŽ² SEED: 789\n","================================================================================\n","Samples: 500 real + 3000 fake\n"]},{"output_type":"stream","name":"stderr","text":["Seed 789: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 55/55 [16:24<00:00, 17.89s/it]\n"]},{"output_type":"stream","name":"stdout","text":["\n","âœ… Results: Acc=0.8526, AUC=0.8498, F1=0.9102\n","\n","================================================================================\n","ðŸŽ² SEED: 2024\n","================================================================================\n","Samples: 500 real + 3000 fake\n"]},{"output_type":"stream","name":"stderr","text":["Seed 2024: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 55/55 [16:21<00:00, 17.85s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","âœ… Results: Acc=0.8491, AUC=0.8439, F1=0.9079\n","\n","================================================================================\n","ðŸ“Š FINAL STATISTICS\n","================================================================================\n","ACCURACY    : 84.87% Â± 0.23%\n","AUC         : 84.38% Â± 0.36%\n","PRECISION   : 95.17% Â± 0.01%\n","RECALL      : 86.75% Â± 0.27%\n","F1_SCORE    : 90.76% Â± 0.16%\n","\n","âœ… Saved: /content/lstm_fakeavceleb_5runs.csv\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]}]}