{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4","authorship_tag":"ABX9TyN3WGiUpV4ItdOIxUnJASy6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import sys\n","print(sys.version)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qwFVGOT-CMR4","executionInfo":{"status":"ok","timestamp":1759578322015,"user_tz":-360,"elapsed":29,"user":{"displayName":"Affshafee Rahman","userId":"16838251676656766360"}},"outputId":"daa2422a-33fd-4578-a185-dc339f1110a7"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["3.12.11 (main, Jun  4 2025, 08:56:18) [GCC 11.4.0]\n"]}]},{"cell_type":"code","source":["# --- 0. Install Prerequisites ---\n","!pip install mamba-ssm causal-conv1d ffmpeg-python --quiet\n","print(\"✅ Libraries installed.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y4K2TPJaCZRk","executionInfo":{"status":"ok","timestamp":1759578600089,"user_tz":-360,"elapsed":277129,"user":{"displayName":"Affshafee Rahman","userId":"16838251676656766360"}},"outputId":"454000f9-dba8-4d43-c04d-f59001cacd22"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/113.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.8/113.8 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[33m  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /packages/03/e5/2d2b2e067234c0022ff491ff8e574ca0c67094b2deb61249a2be21789cbb/causal_conv1d-1.5.2.tar.gz\u001b[0m\u001b[33m\n","\u001b[0m  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for mamba-ssm (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for causal-conv1d (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","✅ Libraries installed.\n"]}]},{"cell_type":"code","source":["import os\n","from google.colab import drive\n","\n","print(\"\\n\" + \"=\"*80)\n","print(\"📂 STEP 1: UNPACKING DATASET FROM .ZIP FILE\")\n","print(\"=\"*80)\n","try:\n","    # 1. Mount your Google Drive\n","    drive.mount('/content/drive', force_remount=True)\n","\n","    # 2. Define paths\n","    # IMPORTANT: Make sure your new zip file is named 'AVLips.zip' in your Drive\n","    ZIP_FILE_PATH = \"/content/drive/MyDrive/CSE400 codes - 144/AVLips.zip\"\n","    EXTRACT_TO_DIR = \"/content/AVLips_data/\"\n","    os.makedirs(EXTRACT_TO_DIR, exist_ok=True)\n","\n","    # 3. Unpack the dataset using the 'unzip' command\n","    if not os.path.exists(os.path.join(EXTRACT_TO_DIR, \"0_real\")):\n","        print(f\"🚀 Starting to unpack '{ZIP_FILE_PATH}'...\")\n","        # Use the 'unzip' command with -q for quiet mode and -d for destination\n","        !unzip -q \"{ZIP_FILE_PATH}\" -d \"{EXTRACT_TO_DIR}\"\n","        print(\"✅ Unpacking complete!\")\n","    else:\n","        print(\"✅ Dataset already unpacked.\")\n","except Exception as e:\n","    print(f\"❌ An error occurred during unpacking: {e}\")\n","    raise\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PhCV-5p5CXA2","executionInfo":{"status":"ok","timestamp":1759578835235,"user_tz":-360,"elapsed":235140,"user":{"displayName":"Affshafee Rahman","userId":"16838251676656766360"}},"outputId":"df1ae1d6-92d9-417b-d902-c321c56cfd74"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","================================================================================\n","📂 STEP 1: UNPACKING DATASET FROM .ZIP FILE\n","================================================================================\n","Mounted at /content/drive\n","🚀 Starting to unpack '/content/drive/MyDrive/CSE400 codes - 144/AVLips.zip'...\n","✅ Unpacking complete!\n"]}]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IQ7gJMJUCIRq","executionInfo":{"status":"ok","timestamp":1759582350155,"user_tz":-360,"elapsed":3514912,"user":{"displayName":"Affshafee Rahman","userId":"16838251676656766360"}},"outputId":"c3555781-d461-4f6e-b295-0823d39fd5d6"},"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Libraries imported successfully.\n","\n","================================================================================\n","🚀 V1 MAMBA ABLATION STUDY - BATCH 1\n","Testing: V1a (64,64), V1b (96,96), V1d (160,160)\n","================================================================================\n","\n","\n","================================================================================\n","STEP 1: PREPARING FILE LISTS\n","================================================================================\n","🔥 Sampling 2000 videos per class...\n","Total: 4000 | Train: 2800 | Val: 600 | Test: 600\n","\n","================================================================================\n","STEP 2: CACHING DATA (ONCE FOR ALL VARIANTS)\n","================================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Caching Train: 100%|██████████| 44/44 [29:12<00:00, 39.83s/it]\n","Caching Val: 100%|██████████| 10/10 [06:26<00:00, 38.60s/it]\n","Caching Test: 100%|██████████| 10/10 [05:27<00:00, 32.76s/it]\n"]},{"output_type":"stream","name":"stdout","text":["✅ Cached - Train: 1375, Val: 282, Test: 278\n","\n","================================================================================\n","📊 VARIANT 1/3: V1a\n","   Visual d_model=64, Audio d_model=64\n","   Started at: 12:35:17\n","================================================================================\n","\n","Downloading: \"https://download.pytorch.org/models/mobilenet_v3_small-047dcff4.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v3_small-047dcff4.pth\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 9.83M/9.83M [00:00<00:00, 189MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Model: 2,026,433 params (2.026M), 7.82 MB\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 22/22 [05:37<00:00, 15.35s/it, loss=0.6991]\n","Validating: 100%|██████████| 5/5 [00:05<00:00,  1.01s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/25 - Train: 0.6914, Val: 0.6874 ✅\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 22/22 [00:09<00:00,  2.37it/s, loss=0.6689]\n","Validating: 100%|██████████| 5/5 [00:01<00:00,  2.69it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2/25 - Train: 0.6863, Val: 0.6857 ✅\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 22/22 [00:08<00:00,  2.49it/s, loss=0.6007]\n","Validating: 100%|██████████| 5/5 [00:01<00:00,  2.63it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3/25 - Train: 0.6460, Val: 0.6641 ✅\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 22/22 [00:08<00:00,  2.51it/s, loss=0.5078]\n","Validating: 100%|██████████| 5/5 [00:02<00:00,  2.41it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4/25 - Train: 0.5654, Val: 0.7855\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 22/22 [00:09<00:00,  2.43it/s, loss=0.5613]\n","Validating: 100%|██████████| 5/5 [00:01<00:00,  2.52it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5/25 - Train: 0.4606, Val: 1.7441\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 22/22 [00:08<00:00,  2.59it/s, loss=0.3851]\n","Validating: 100%|██████████| 5/5 [00:01<00:00,  2.56it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 6/25 - Train: 0.3836, Val: 1.4326\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 22/22 [00:08<00:00,  2.47it/s, loss=0.3101]\n","Validating: 100%|██████████| 5/5 [00:01<00:00,  2.72it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 7/25 - Train: 0.3259, Val: 0.9768\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 22/22 [00:09<00:00,  2.43it/s, loss=0.2519]\n","Validating: 100%|██████████| 5/5 [00:01<00:00,  2.56it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 8/25 - Train: 0.2967, Val: 0.8260\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 22/22 [00:08<00:00,  2.46it/s, loss=0.2801]\n","Validating: 100%|██████████| 5/5 [00:01<00:00,  2.53it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 9/25 - Train: 0.2795, Val: 1.1009\n","Early stopping at epoch 9\n"]},{"output_type":"stream","name":"stderr","text":["Evaluating: 100%|██████████| 5/5 [00:52<00:00, 10.50s/it]\n"]},{"output_type":"stream","name":"stdout","text":["\n","✅ V1a Results:\n","   Accuracy: 52.52%, AUC: 69.55%, Loss Gap: -0.8214\n","   Saved to: /content/v1_ablation_batch1_results.csv\n","\n","================================================================================\n","📊 VARIANT 2/3: V1b\n","   Visual d_model=96, Audio d_model=96\n","   Started at: 12:43:19\n","================================================================================\n","\n","Model: 2,150,785 params (2.151M), 8.30 MB\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 22/22 [00:09<00:00,  2.36it/s, loss=0.6877]\n","Validating: 100%|██████████| 5/5 [00:01<00:00,  2.70it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/25 - Train: 0.6909, Val: 0.6900 ✅\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 22/22 [00:09<00:00,  2.42it/s, loss=0.5928]\n","Validating: 100%|██████████| 5/5 [00:02<00:00,  2.49it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2/25 - Train: 0.6617, Val: 0.6861 ✅\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 22/22 [00:08<00:00,  2.52it/s, loss=0.5785]\n","Validating: 100%|██████████| 5/5 [00:01<00:00,  2.68it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3/25 - Train: 0.6143, Val: 0.6879\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 22/22 [00:08<00:00,  2.50it/s, loss=0.6062]\n","Validating: 100%|██████████| 5/5 [00:01<00:00,  2.60it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4/25 - Train: 0.5725, Val: 0.6700 ✅\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 22/22 [00:09<00:00,  2.43it/s, loss=0.4093]\n","Validating: 100%|██████████| 5/5 [00:01<00:00,  2.54it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5/25 - Train: 0.4796, Val: 0.7849\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 22/22 [00:09<00:00,  2.42it/s, loss=0.5182]\n","Validating: 100%|██████████| 5/5 [00:01<00:00,  2.63it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 6/25 - Train: 0.4025, Val: 0.6864\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 22/22 [00:09<00:00,  2.39it/s, loss=0.3415]\n","Validating: 100%|██████████| 5/5 [00:01<00:00,  2.51it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 7/25 - Train: 0.3433, Val: 0.7255\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 22/22 [00:09<00:00,  2.37it/s, loss=0.2781]\n","Validating: 100%|██████████| 5/5 [00:01<00:00,  2.63it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 8/25 - Train: 0.3031, Val: 0.7386\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 22/22 [00:08<00:00,  2.53it/s, loss=0.2716]\n","Validating: 100%|██████████| 5/5 [00:01<00:00,  2.57it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 9/25 - Train: 0.2732, Val: 0.7588\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 22/22 [00:08<00:00,  2.48it/s, loss=0.2500]\n","Validating: 100%|██████████| 5/5 [00:01<00:00,  2.57it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 10/25 - Train: 0.2580, Val: 0.6481 ✅\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 22/22 [00:08<00:00,  2.52it/s, loss=0.2360]\n","Validating: 100%|██████████| 5/5 [00:01<00:00,  2.57it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 11/25 - Train: 0.2494, Val: 0.6122 ✅\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 22/22 [00:08<00:00,  2.50it/s, loss=0.2359]\n","Validating: 100%|██████████| 5/5 [00:01<00:00,  2.59it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 12/25 - Train: 0.2413, Val: 0.6409\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 22/22 [00:09<00:00,  2.43it/s, loss=0.2282]\n","Validating: 100%|██████████| 5/5 [00:01<00:00,  2.58it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 13/25 - Train: 0.2374, Val: 0.5351 ✅\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 22/22 [00:09<00:00,  2.40it/s, loss=0.2226]\n","Validating: 100%|██████████| 5/5 [00:01<00:00,  2.51it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 14/25 - Train: 0.2333, Val: 0.5140 ✅\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 22/22 [00:08<00:00,  2.49it/s, loss=0.2314]\n","Validating: 100%|██████████| 5/5 [00:01<00:00,  2.64it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 15/25 - Train: 0.2295, Val: 0.4657 ✅\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 22/22 [00:08<00:00,  2.51it/s, loss=0.2192]\n","Validating: 100%|██████████| 5/5 [00:01<00:00,  2.52it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 16/25 - Train: 0.2278, Val: 0.4602 ✅\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 22/22 [00:08<00:00,  2.52it/s, loss=0.2201]\n","Validating: 100%|██████████| 5/5 [00:01<00:00,  2.60it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 17/25 - Train: 0.2250, Val: 0.4427 ✅\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 22/22 [00:08<00:00,  2.48it/s, loss=0.2277]\n","Validating: 100%|██████████| 5/5 [00:01<00:00,  2.62it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 18/25 - Train: 0.2263, Val: 0.4138 ✅\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 22/22 [00:09<00:00,  2.42it/s, loss=0.2128]\n","Validating: 100%|██████████| 5/5 [00:01<00:00,  2.56it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 19/25 - Train: 0.2228, Val: 0.3970 ✅\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 22/22 [00:09<00:00,  2.34it/s, loss=0.2164]\n","Validating: 100%|██████████| 5/5 [00:01<00:00,  2.61it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 20/25 - Train: 0.2205, Val: 0.4325\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 22/22 [00:08<00:00,  2.46it/s, loss=0.2101]\n","Validating: 100%|██████████| 5/5 [00:01<00:00,  2.66it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 21/25 - Train: 0.2200, Val: 0.4030\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 22/22 [00:08<00:00,  2.46it/s, loss=0.2284]\n","Validating: 100%|██████████| 5/5 [00:01<00:00,  2.58it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 22/25 - Train: 0.2185, Val: 0.4064\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 22/22 [00:08<00:00,  2.53it/s, loss=0.2186]\n","Validating: 100%|██████████| 5/5 [00:02<00:00,  2.45it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 23/25 - Train: 0.2171, Val: 0.4070\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 22/22 [00:08<00:00,  2.45it/s, loss=0.2213]\n","Validating: 100%|██████████| 5/5 [00:01<00:00,  2.67it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 24/25 - Train: 0.2184, Val: 0.4067\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 22/22 [00:08<00:00,  2.45it/s, loss=0.2242]\n","Validating: 100%|██████████| 5/5 [00:01<00:00,  2.56it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 25/25 - Train: 0.2160, Val: 0.4005\n","Early stopping at epoch 25\n"]},{"output_type":"stream","name":"stderr","text":["Evaluating: 100%|██████████| 5/5 [00:02<00:00,  2.45it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","✅ V1b Results:\n","   Accuracy: 87.05%, AUC: 96.39%, Loss Gap: -0.1845\n","   Saved to: /content/v1_ablation_batch1_results.csv\n","\n","================================================================================\n","📊 VARIANT 3/3: V1d\n","   Visual d_model=160, Audio d_model=160\n","   Started at: 12:47:55\n","================================================================================\n","\n","Model: 2,476,289 params (2.476M), 9.54 MB\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 22/22 [00:09<00:00,  2.42it/s, loss=0.6940]\n","Validating: 100%|██████████| 5/5 [00:01<00:00,  2.67it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/25 - Train: 0.6949, Val: 0.6976 ✅\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 22/22 [00:09<00:00,  2.41it/s, loss=0.5948]\n","Validating: 100%|██████████| 5/5 [00:01<00:00,  2.64it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2/25 - Train: 0.6670, Val: 0.6927 ✅\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 22/22 [00:09<00:00,  2.44it/s, loss=0.5950]\n","Validating: 100%|██████████| 5/5 [00:01<00:00,  2.55it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3/25 - Train: 0.6052, Val: 0.6628 ✅\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 22/22 [00:08<00:00,  2.49it/s, loss=0.5667]\n","Validating: 100%|██████████| 5/5 [00:02<00:00,  2.41it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4/25 - Train: 0.5468, Val: 0.6363 ✅\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 22/22 [00:08<00:00,  2.51it/s, loss=0.5204]\n","Validating: 100%|██████████| 5/5 [00:01<00:00,  2.50it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5/25 - Train: 0.5001, Val: 0.6173 ✅\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 22/22 [00:09<00:00,  2.44it/s, loss=0.4110]\n","Validating: 100%|██████████| 5/5 [00:02<00:00,  2.48it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 6/25 - Train: 0.4465, Val: 0.5794 ✅\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 22/22 [00:08<00:00,  2.53it/s, loss=0.2539]\n","Validating: 100%|██████████| 5/5 [00:01<00:00,  2.57it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 7/25 - Train: 0.3553, Val: 0.5106 ✅\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 22/22 [00:09<00:00,  2.41it/s, loss=0.2797]\n","Validating: 100%|██████████| 5/5 [00:01<00:00,  2.67it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 8/25 - Train: 0.2954, Val: 0.4710 ✅\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 22/22 [00:08<00:00,  2.50it/s, loss=0.2444]\n","Validating: 100%|██████████| 5/5 [00:01<00:00,  2.52it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 9/25 - Train: 0.2685, Val: 0.4486 ✅\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 22/22 [00:08<00:00,  2.52it/s, loss=0.2633]\n","Validating: 100%|██████████| 5/5 [00:02<00:00,  2.46it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 10/25 - Train: 0.2535, Val: 0.4222 ✅\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 22/22 [00:08<00:00,  2.47it/s, loss=0.2445]\n","Validating: 100%|██████████| 5/5 [00:01<00:00,  2.57it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 11/25 - Train: 0.2348, Val: 0.4813\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 22/22 [00:08<00:00,  2.51it/s, loss=0.2099]\n","Validating: 100%|██████████| 5/5 [00:01<00:00,  2.65it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 12/25 - Train: 0.2368, Val: 0.3973 ✅\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 22/22 [00:08<00:00,  2.48it/s, loss=0.2234]\n","Validating: 100%|██████████| 5/5 [00:01<00:00,  2.56it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 13/25 - Train: 0.2249, Val: 0.3916 ✅\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 22/22 [00:08<00:00,  2.45it/s, loss=0.2135]\n","Validating: 100%|██████████| 5/5 [00:01<00:00,  2.63it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 14/25 - Train: 0.2202, Val: 0.3403 ✅\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 22/22 [00:09<00:00,  2.39it/s, loss=0.2275]\n","Validating: 100%|██████████| 5/5 [00:01<00:00,  2.56it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 15/25 - Train: 0.2187, Val: 0.3655\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 22/22 [00:08<00:00,  2.48it/s, loss=0.2168]\n","Validating: 100%|██████████| 5/5 [00:02<00:00,  2.48it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 16/25 - Train: 0.2159, Val: 0.3509\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 22/22 [00:08<00:00,  2.50it/s, loss=0.2102]\n","Validating: 100%|██████████| 5/5 [00:01<00:00,  2.60it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 17/25 - Train: 0.2152, Val: 0.3758\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 22/22 [00:08<00:00,  2.51it/s, loss=0.2117]\n","Validating: 100%|██████████| 5/5 [00:01<00:00,  2.64it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 18/25 - Train: 0.2123, Val: 0.3700\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 22/22 [00:08<00:00,  2.48it/s, loss=0.2175]\n","Validating: 100%|██████████| 5/5 [00:02<00:00,  2.49it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 19/25 - Train: 0.2104, Val: 0.3663\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 22/22 [00:08<00:00,  2.45it/s, loss=0.2125]\n","Validating: 100%|██████████| 5/5 [00:01<00:00,  2.51it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 20/25 - Train: 0.2109, Val: 0.3347 ✅\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 22/22 [00:08<00:00,  2.51it/s, loss=0.2147]\n","Validating: 100%|██████████| 5/5 [00:01<00:00,  2.61it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 21/25 - Train: 0.2102, Val: 0.3079 ✅\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 22/22 [00:08<00:00,  2.55it/s, loss=0.2062]\n","Validating: 100%|██████████| 5/5 [00:02<00:00,  2.45it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 22/25 - Train: 0.2094, Val: 0.2919 ✅\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 22/22 [00:08<00:00,  2.57it/s, loss=0.2117]\n","Validating: 100%|██████████| 5/5 [00:02<00:00,  2.47it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 23/25 - Train: 0.2110, Val: 0.2837 ✅\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 22/22 [00:08<00:00,  2.50it/s, loss=0.2137]\n","Validating: 100%|██████████| 5/5 [00:01<00:00,  2.57it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 24/25 - Train: 0.2089, Val: 0.2816 ✅\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 22/22 [00:08<00:00,  2.47it/s, loss=0.2121]\n","Validating: 100%|██████████| 5/5 [00:01<00:00,  2.62it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 25/25 - Train: 0.2089, Val: 0.2782 ✅\n"]},{"output_type":"stream","name":"stderr","text":["Evaluating: 100%|██████████| 5/5 [00:02<00:00,  2.45it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","✅ V1d Results:\n","   Accuracy: 94.60%, AUC: 99.12%, Loss Gap: -0.0692\n","   Saved to: /content/v1_ablation_batch1_results.csv\n","\n","================================================================================\n","🎉 BATCH 1 COMPLETED!\n","================================================================================\n","Total time: 0.97 hours\n","\n","Results Summary:\n","  variant  params_M  accuracy       auc  loss_gap\n","0     V1a  2.026433  0.525180  0.695465 -0.821418\n","1     V1b  2.150785  0.870504  0.963885 -0.184515\n","2     V1d  2.476289  0.946043  0.991179 -0.069248\n","\n","📁 Results saved to: /content/v1_ablation_batch1_results.csv\n"]}],"source":["# -*- coding: utf-8 -*-\n","\n","\"\"\"\n","V1 MAMBA ABLATION STUDY - BATCH 1\n","Testing V1a (64,64), V1b (96,96), V1d (160,160)\n","\"\"\"\n","\n","# --- 1. IMPORTS ---\n","import os\n","import cv2\n","import time\n","import torch\n","import librosa\n","import numpy as np\n","import pandas as pd\n","import torch.nn as nn\n","from pathlib import Path\n","from tqdm import tqdm\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","import matplotlib.pyplot as plt\n","import torch.nn.functional as F\n","import torchvision.models as models\n","from mamba_ssm import Mamba\n","from torch.cuda.amp import autocast, GradScaler\n","import torchvision.transforms as transforms\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report, roc_auc_score\n","import warnings\n","\n","warnings.filterwarnings('ignore')\n","torch.backends.cudnn.benchmark = True\n","print(\"✅ Libraries imported successfully.\")\n","\n","# --- 2. CONFIGURATION ---\n","class Config:\n","    def __init__(self, vis_d_model=128, aud_d_model=128):\n","        self.data_dir = \"/content/AVLips_data/AVLips\"\n","        self.model_save_dir = \"/content/models/\"\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        os.makedirs(self.model_save_dir, exist_ok=True)\n","\n","        self.use_sampling = True\n","        self.num_samples_per_class = 2000\n","\n","        # Visual Stream\n","        self.vis_image_size = (128, 128)\n","        self.vis_num_frames = 16\n","        self.vis_cnn_feature_dim = 576\n","        self.vis_mamba_d_model = vis_d_model  # CONFIGURABLE\n","\n","        # Audio Stream\n","        self.aud_sample_rate = 16000\n","        self.aud_num_chunks = 5\n","        self.aud_chunk_duration = 1.0\n","        self.aud_n_mels = 128\n","        self.aud_cnn_feature_dim = 576\n","        self.aud_mamba_d_model = aud_d_model  # CONFIGURABLE\n","\n","        # Training\n","        self.batch_size = 64\n","        self.accumulation_steps = 4\n","        self.epochs = 25\n","        self.learning_rate = 5e-4\n","        self.weight_decay = 0.05\n","        self.patience = 6\n","\n","# --- 3. LABEL SMOOTHING LOSS ---\n","class LabelSmoothingBCELoss(nn.Module):\n","    def __init__(self, smoothing=0.1):\n","        super().__init__()\n","        self.smoothing = smoothing\n","\n","    def forward(self, pred, target):\n","        target = target * (1 - self.smoothing) + 0.5 * self.smoothing\n","        return F.binary_cross_entropy_with_logits(pred, target)\n","\n","# --- 4. DATA PROCESSING ---\n","def process_visual_stream(video_path: str, config: Config):\n","    cap = cv2.VideoCapture(video_path)\n","    if not cap.isOpened():\n","        return None\n","\n","    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","    if total_frames < config.vis_num_frames:\n","        return None\n","\n","    frame_indices = np.linspace(0, total_frames - 1, config.vis_num_frames, dtype=int)\n","    frames = []\n","    face_detector = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n","\n","    for i in frame_indices:\n","        cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n","        ret, frame = cap.read()\n","        if not ret:\n","            continue\n","\n","        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n","        faces = face_detector.detectMultiScale(gray, 1.1, 4)\n","\n","        if len(faces) > 0:\n","            (x, y, w, h) = faces[0]\n","            mouth_crop = frame[y + int(h * 0.6):y + h, x + int(w * 0.25):x + int(w * 0.75)]\n","            if mouth_crop.size > 0:\n","                resized_crop = cv2.resize(mouth_crop, config.vis_image_size)\n","                resized_crop_rgb = cv2.cvtColor(resized_crop, cv2.COLOR_BGR2RGB)\n","                frames.append(resized_crop_rgb)\n","\n","    cap.release()\n","    return np.stack(frames) if len(frames) == config.vis_num_frames else None\n","\n","def process_audio_stream(video_path: str, config: Config):\n","    try:\n","        parts = Path(video_path).parts\n","        audio_filename = Path(video_path).stem + \".wav\"\n","        label_folder = parts[-2]\n","        base_data_dir = str(Path(video_path).parent.parent)\n","        audio_path = os.path.join(base_data_dir, \"wav\", label_folder, audio_filename)\n","\n","        y, sr = librosa.load(audio_path, sr=config.aud_sample_rate)\n","        total_samples = int(config.aud_chunk_duration * config.aud_num_chunks * sr)\n","\n","        if len(y) < total_samples:\n","            y = np.pad(y, (0, total_samples - len(y)), mode='constant')\n","        else:\n","            y = y[:total_samples]\n","\n","        samples_per_chunk = int(config.aud_chunk_duration * sr)\n","        mel_list = []\n","\n","        for i in range(config.aud_num_chunks):\n","            chunk = y[i*samples_per_chunk : (i+1)*samples_per_chunk]\n","            mel = librosa.feature.melspectrogram(y=chunk, sr=sr, n_mels=config.aud_n_mels)\n","            mel_db = librosa.power_to_db(mel, ref=np.max)\n","            mel_db = (mel_db - mel_db.mean()) / (mel_db.std() + 1e-9)\n","            mel_list.append(torch.tensor(mel_db, dtype=torch.float32))\n","\n","        return torch.stack(mel_list, axis=0)\n","    except Exception:\n","        return None\n","\n","class DualStreamDataset(Dataset):\n","    def __init__(self, file_paths, labels, config):\n","        self.file_paths = file_paths\n","        self.labels = labels\n","        self.config = config\n","\n","    def __len__(self):\n","        return len(self.file_paths)\n","\n","    def __getitem__(self, idx):\n","        video_path = self.file_paths[idx]\n","        label = self.labels[idx]\n","\n","        try:\n","            visual_frames_hwc = process_visual_stream(video_path, self.config)\n","            if visual_frames_hwc is None:\n","                return None\n","\n","            visual_frames_tchw = visual_frames_hwc.transpose(0, 3, 1, 2)\n","            audio_mels = process_audio_stream(video_path, self.config)\n","            if audio_mels is None:\n","                return None\n","\n","            audio_tensor = audio_mels.unsqueeze(1)\n","            return (visual_frames_tchw, audio_tensor), torch.tensor(label, dtype=torch.float32)\n","        except Exception:\n","            return None\n","\n","class RAMCachedDataset(Dataset):\n","    def __init__(self, data, labels, transform=None):\n","        self.data = data\n","        self.labels = labels\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def __getitem__(self, idx):\n","        visual_frames_np, audio_tensor = self.data[idx]\n","        label = self.labels[idx]\n","\n","        if self.transform:\n","            augmented_frames = []\n","            for frame_np in visual_frames_np:\n","                frame_hwc = frame_np.transpose(1, 2, 0)\n","                augmented_frames.append(self.transform(frame_hwc))\n","            visual_tensor = torch.stack(augmented_frames)\n","        else:\n","            visual_tensor = torch.from_numpy(visual_frames_np).float()\n","\n","        return (visual_tensor, audio_tensor), label\n","\n","# --- 5. MODEL ARCHITECTURE ---\n","class VisualStream_MobileNetV3Small(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        mobilenet = models.mobilenet_v3_small(weights=models.MobileNet_V3_Small_Weights.DEFAULT)\n","        self.cnn_features = mobilenet.features\n","        self.avgpool = mobilenet.avgpool\n","\n","        self.proj = nn.Linear(config.vis_cnn_feature_dim, config.vis_mamba_d_model)\n","        self.proj_dropout = nn.Dropout(0.3)\n","        self.mamba = Mamba(d_model=config.vis_mamba_d_model, d_state=16, d_conv=4, expand=2)\n","        self.mamba_dropout = nn.Dropout(0.2)\n","\n","    def forward(self, x):\n","        b, t, c, h, w = x.shape\n","        x = x.view(b * t, c, h, w)\n","\n","        features = self.cnn_features(x)\n","        features = self.avgpool(features)\n","        features = features.view(b, t, -1)\n","\n","        projected_features = self.proj_dropout(self.proj(features))\n","        temporal_out = self.mamba(projected_features)\n","        temporal_out = self.mamba_dropout(temporal_out)\n","\n","        return temporal_out[:, -1, :]\n","\n","class AudioStream_MobileNetV3Small(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        mobilenet = models.mobilenet_v3_small(weights=models.MobileNet_V3_Small_Weights.DEFAULT)\n","        self.cnn_features = mobilenet.features\n","        self.avgpool = mobilenet.avgpool\n","\n","        self.proj = nn.Linear(config.aud_cnn_feature_dim, config.aud_mamba_d_model)\n","        self.proj_dropout = nn.Dropout(0.3)\n","        self.mamba = Mamba(d_model=config.aud_mamba_d_model, d_state=16, d_conv=4, expand=2)\n","        self.mamba_dropout = nn.Dropout(0.2)\n","\n","    def forward(self, x):\n","        b, t, c, h, w = x.shape\n","        x = x.view(b * t, c, h, w).repeat(1, 3, 1, 1)\n","\n","        features = self.cnn_features(x)\n","        features = self.avgpool(features)\n","        features = features.view(b, t, -1)\n","\n","        projected_features = self.proj_dropout(self.proj(features))\n","        temporal_out = self.mamba(projected_features)\n","        temporal_out = self.mamba_dropout(temporal_out)\n","\n","        return temporal_out[:, -1, :]\n","\n","class FusionModel_V1(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.visual_stream = VisualStream_MobileNetV3Small(config)\n","        self.audio_stream = AudioStream_MobileNetV3Small(config)\n","\n","        fusion_input_dim = config.vis_mamba_d_model + config.aud_mamba_d_model\n","        self.fusion_head = nn.Sequential(\n","            nn.Linear(fusion_input_dim, 256),\n","            nn.ReLU(),\n","            nn.Dropout(0.6),\n","            nn.Linear(256, 1)\n","        )\n","\n","    def forward(self, visual_input, audio_input):\n","        visual_features = self.visual_stream(visual_input)\n","        audio_features = self.audio_stream(audio_input)\n","        fused_features = torch.cat((visual_features, audio_features), dim=1)\n","        return self.fusion_head(fused_features)\n","\n","# --- 6. UTILITY FUNCTIONS ---\n","def count_parameters(model):\n","    total = sum(p.numel() for p in model.parameters())\n","    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","    return total, trainable\n","\n","def get_model_size_mb(model):\n","    param_size = sum(p.nelement() * p.element_size() for p in model.parameters())\n","    buffer_size = sum(b.nelement() * b.element_size() for b in model.buffers())\n","    return (param_size + buffer_size) / (1024 ** 2)\n","\n","# --- 7. TRAINING FUNCTIONS ---\n","def train_one_epoch(model, loader, optimizer, criterion, scaler, config):\n","    model.train()\n","    total_loss = 0\n","    pbar = tqdm(loader, desc=\"Training\")\n","\n","    for i, ((visual_data, audio_data), labels) in enumerate(pbar):\n","        visual_data = visual_data.to(config.device, non_blocking=True)\n","        audio_data = audio_data.to(config.device, non_blocking=True)\n","        labels = labels.to(config.device, non_blocking=True).unsqueeze(1).float()\n","\n","        with autocast():\n","            outputs = model(visual_data, audio_data)\n","            loss = criterion(outputs, labels)\n","            loss = loss / config.accumulation_steps\n","\n","        scaler.scale(loss).backward()\n","\n","        if (i + 1) % config.accumulation_steps == 0:\n","            scaler.unscale_(optimizer)\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","            scaler.step(optimizer)\n","            scaler.update()\n","            optimizer.zero_grad(set_to_none=True)\n","\n","        total_loss += loss.item() * config.accumulation_steps\n","        pbar.set_postfix({'loss': f\"{loss.item() * config.accumulation_steps:.4f}\"})\n","\n","    return total_loss / len(loader)\n","\n","def validate_one_epoch(model, loader, criterion, config):\n","    model.eval()\n","    total_loss = 0\n","\n","    with torch.no_grad():\n","        for (visual_data, audio_data), labels in tqdm(loader, desc=\"Validating\"):\n","            visual_data = visual_data.to(config.device, non_blocking=True)\n","            audio_data = audio_data.to(config.device, non_blocking=True)\n","            labels = labels.to(config.device, non_blocking=True).unsqueeze(1).float()\n","\n","            with autocast():\n","                outputs = model(visual_data, audio_data)\n","                loss = criterion(outputs, labels)\n","\n","            total_loss += loss.item()\n","\n","    return total_loss / len(loader)\n","\n","def evaluate_model(model, test_loader, config):\n","    model.eval()\n","    all_labels, all_preds = [], []\n","\n","    with torch.no_grad():\n","        for (visual_data, audio_data), labels in tqdm(test_loader, desc=\"Evaluating\"):\n","            visual_data = visual_data.to(config.device)\n","            audio_data = audio_data.to(config.device)\n","\n","            outputs = model(visual_data, audio_data)\n","            all_preds.extend(torch.sigmoid(outputs).cpu().numpy())\n","            all_labels.extend(labels.numpy())\n","\n","    all_preds = np.array(all_preds).flatten()\n","    all_labels = np.array(all_labels).flatten()\n","    preds_binary = (all_preds > 0.5).astype(int)\n","\n","    accuracy = (preds_binary == all_labels).mean()\n","    auc_score = roc_auc_score(all_labels, all_preds)\n","\n","    return accuracy, auc_score, all_labels, preds_binary\n","\n","# --- 8. MAIN EXECUTION ---\n","def main():\n","    print(\"\\n\" + \"=\"*80)\n","    print(\"🚀 V1 MAMBA ABLATION STUDY - BATCH 1\")\n","    print(\"Testing: V1a (64,64), V1b (96,96), V1d (160,160)\")\n","    print(\"=\"*80 + \"\\n\")\n","\n","    start_time = time.time()\n","\n","    # Initialize base config for data caching\n","    base_config = Config()\n","\n","    print(\"\\n\" + \"=\"*80 + \"\\nSTEP 1: PREPARING FILE LISTS\\n\" + \"=\"*80)\n","    real_dir = os.path.join(base_config.data_dir, \"0_real\")\n","    fake_dir = os.path.join(base_config.data_dir, \"1_fake\")\n","\n","    all_real = [os.path.join(real_dir, f) for f in os.listdir(real_dir) if f.endswith('.mp4')]\n","    all_fake = [os.path.join(fake_dir, f) for f in os.listdir(fake_dir) if f.endswith('.mp4')]\n","\n","    if base_config.use_sampling:\n","        print(f\"🔥 Sampling {base_config.num_samples_per_class} videos per class...\")\n","        real_files = np.random.choice(all_real, base_config.num_samples_per_class, replace=False).tolist()\n","        fake_files = np.random.choice(all_fake, base_config.num_samples_per_class, replace=False).tolist()\n","    else:\n","        real_files, fake_files = all_real, all_fake\n","\n","    all_files = real_files + fake_files\n","    labels = [0] * len(real_files) + [1] * len(fake_files)\n","\n","    train_files, test_files, train_labels, test_labels = train_test_split(\n","        all_files, labels, test_size=0.3, random_state=42, stratify=labels)\n","    val_files, test_files, val_labels, test_labels = train_test_split(\n","        test_files, test_labels, test_size=0.5, random_state=42, stratify=test_labels)\n","\n","    print(f\"Total: {len(all_files)} | Train: {len(train_files)} | Val: {len(val_files)} | Test: {len(test_files)}\")\n","\n","    print(\"\\n\" + \"=\"*80 + \"\\nSTEP 2: CACHING DATA (ONCE FOR ALL VARIANTS)\\n\" + \"=\"*80)\n","\n","    def collate_fn_skip_errors(batch):\n","        batch = list(filter(lambda x: x is not None, batch))\n","        return torch.utils.data.dataloader.default_collate(batch) if batch else (None, None)\n","\n","    def cache_data(files, labels, desc):\n","        dataset = DualStreamDataset(files, labels, base_config)\n","        loader = DataLoader(dataset, batch_size=base_config.batch_size, num_workers=os.cpu_count(), collate_fn=collate_fn_skip_errors)\n","        cached_data, cached_labels = [], []\n","\n","        for data, batch_labels in tqdm(loader, desc=f\"Caching {desc}\"):\n","            if data is not None:\n","                visual_batch, audio_batch = data\n","                for i in range(visual_batch.shape[0]):\n","                    cached_data.append((visual_batch[i].numpy(), audio_batch[i]))\n","                    cached_labels.append(batch_labels[i])\n","\n","        return cached_data, torch.tensor(cached_labels)\n","\n","    cached_train_data, cached_train_labels = cache_data(train_files, train_labels, \"Train\")\n","    cached_val_data, cached_val_labels = cache_data(val_files, val_labels, \"Val\")\n","    cached_test_data, cached_test_labels = cache_data(test_files, test_labels, \"Test\")\n","\n","    print(f\"✅ Cached - Train: {len(cached_train_data)}, Val: {len(cached_val_data)}, Test: {len(cached_test_data)}\")\n","\n","    # Define transforms\n","    val_test_transform = transforms.Compose([\n","        transforms.ToPILImage(),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","    ])\n","\n","    train_transform = transforms.Compose([\n","        transforms.ToPILImage(),\n","        transforms.RandomHorizontalFlip(p=0.5),\n","        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n","        transforms.RandomRotation(10),\n","        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","    ])\n","\n","    # Variants to test\n","    configs_to_test = [\n","        (64, 64, 'V1a'),\n","        (96, 96, 'V1b'),\n","        (160, 160, 'V1d'),\n","    ]\n","\n","    results = []\n","\n","    for idx, (vis_d, aud_d, variant_name) in enumerate(configs_to_test):\n","        print(\"\\n\" + \"=\"*80)\n","        print(f\"📊 VARIANT {idx+1}/3: {variant_name}\")\n","        print(f\"   Visual d_model={vis_d}, Audio d_model={aud_d}\")\n","        print(f\"   Started at: {time.strftime('%H:%M:%S')}\")\n","        print(\"=\"*80 + \"\\n\")\n","\n","        # Create config for this variant\n","        config = Config(vis_d_model=vis_d, aud_d_model=aud_d)\n","\n","        # Create dataloaders (reusing cached data!)\n","        train_dataset = RAMCachedDataset(cached_train_data, cached_train_labels, transform=train_transform)\n","        val_dataset = RAMCachedDataset(cached_val_data, cached_val_labels, transform=val_test_transform)\n","        test_dataset = RAMCachedDataset(cached_test_data, cached_test_labels, transform=val_test_transform)\n","\n","        train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=os.cpu_count(), pin_memory=True)\n","        val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False, num_workers=os.cpu_count(), pin_memory=True)\n","        test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False, num_workers=os.cpu_count(), pin_memory=True)\n","\n","        # Build model\n","        model = FusionModel_V1(config).to(config.device)\n","        total_params, trainable_params = count_parameters(model)\n","        model_size_mb = get_model_size_mb(model)\n","\n","        print(f\"Model: {total_params:,} params ({total_params/1e6:.3f}M), {model_size_mb:.2f} MB\")\n","\n","        # Training setup\n","        optimizer = optim.AdamW(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n","        criterion = LabelSmoothingBCELoss(smoothing=0.1)\n","        scaler = GradScaler()\n","        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, min_lr=1e-6)\n","\n","        model_path = os.path.join(config.model_save_dir, f'{variant_name.lower()}_best.pth')\n","        best_val_loss = float('inf')\n","        epochs_no_improve = 0\n","        history = {'train_loss': [], 'val_loss': []}\n","\n","        # Training loop\n","        for epoch in range(config.epochs):\n","            train_loss = train_one_epoch(model, train_loader, optimizer, criterion, scaler, config)\n","            val_loss = validate_one_epoch(model, val_loader, criterion, config)\n","\n","            history['train_loss'].append(train_loss)\n","            history['val_loss'].append(val_loss)\n","\n","            scheduler.step(val_loss)\n","\n","            if val_loss < best_val_loss:\n","                best_val_loss = val_loss\n","                epochs_no_improve = 0\n","                torch.save(model.state_dict(), model_path)\n","                print(f\"Epoch {epoch+1}/{config.epochs} - Train: {train_loss:.4f}, Val: {val_loss:.4f} ✅\")\n","            else:\n","                epochs_no_improve += 1\n","                print(f\"Epoch {epoch+1}/{config.epochs} - Train: {train_loss:.4f}, Val: {val_loss:.4f}\")\n","                if epochs_no_improve >= config.patience:\n","                    print(f\"Early stopping at epoch {epoch+1}\")\n","                    break\n","\n","        # Final evaluation\n","        model.load_state_dict(torch.load(model_path))\n","        accuracy, auc_score, all_labels, preds_binary = evaluate_model(model, test_loader, config)\n","        loss_gap = history['train_loss'][-1] - history['val_loss'][-1]\n","\n","        print(f\"\\n✅ {variant_name} Results:\")\n","        print(f\"   Accuracy: {accuracy*100:.2f}%, AUC: {auc_score*100:.2f}%, Loss Gap: {loss_gap:.4f}\")\n","\n","        # Save results\n","        results.append({\n","            'variant': variant_name,\n","            'vis_d_model': vis_d,\n","            'aud_d_model': aud_d,\n","            'params_M': total_params / 1e6,\n","            'size_MB': model_size_mb,\n","            'accuracy': accuracy,\n","            'auc': auc_score,\n","            'loss_gap': loss_gap,\n","            'completed_at': time.strftime('%Y-%m-%d %H:%M:%S')\n","        })\n","\n","        # Save intermediate results\n","        df = pd.DataFrame(results)\n","        df.to_csv('/content/v1_ablation_batch1_results.csv', index=False)\n","        print(f\"   Saved to: /content/v1_ablation_batch1_results.csv\")\n","\n","        # Cleanup\n","        del model\n","        torch.cuda.empty_cache()\n","\n","    # Final summary\n","    total_time = (time.time() - start_time) / 3600\n","    print(\"\\n\" + \"=\"*80)\n","    print(\"🎉 BATCH 1 COMPLETED!\")\n","    print(\"=\"*80)\n","    print(f\"Total time: {total_time:.2f} hours\")\n","    print(\"\\nResults Summary:\")\n","    print(pd.DataFrame(results)[['variant', 'params_M', 'accuracy', 'auc', 'loss_gap']])\n","    print(\"\\n📁 Results saved to: /content/v1_ablation_batch1_results.csv\")\n","\n","if __name__ == '__main__':\n","    main()\n"]}]}