{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4","authorship_tag":"ABX9TyNE+QAauW4QsMhxN9xx+QEK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import sys\n","print(sys.version)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qwFVGOT-CMR4","executionInfo":{"status":"ok","timestamp":1759583143793,"user_tz":-360,"elapsed":17,"user":{"displayName":"Affshafee Rahman","userId":"16838251676656766360"}},"outputId":"42ba0f76-dc03-4d22-d6b1-c2f99012cec8"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["3.12.11 (main, Jun  4 2025, 08:56:18) [GCC 11.4.0]\n"]}]},{"cell_type":"code","source":["# --- 0. Install Prerequisites ---\n","!pip install mamba-ssm causal-conv1d ffmpeg-python --quiet\n","print(\"‚úÖ Libraries installed.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y4K2TPJaCZRk","executionInfo":{"status":"ok","timestamp":1759583565704,"user_tz":-360,"elapsed":419933,"user":{"displayName":"Affshafee Rahman","userId":"16838251676656766360"}},"outputId":"eb33740c-4149-4ebc-908e-3e262163cf4f"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/113.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m113.8/113.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for mamba-ssm (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for causal-conv1d (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","‚úÖ Libraries installed.\n"]}]},{"cell_type":"code","source":["import os\n","from google.colab import drive\n","\n","print(\"\\n\" + \"=\"*80)\n","print(\"üìÇ STEP 1: UNPACKING DATASET FROM .ZIP FILE\")\n","print(\"=\"*80)\n","try:\n","    # 1. Mount your Google Drive\n","    drive.mount('/content/drive', force_remount=True)\n","\n","    # 2. Define paths\n","    # IMPORTANT: Make sure your new zip file is named 'AVLips.zip' in your Drive\n","    ZIP_FILE_PATH = \"/content/drive/MyDrive/CSE400 codes - 144/AVLips.zip\"\n","    EXTRACT_TO_DIR = \"/content/AVLips_data/\"\n","    os.makedirs(EXTRACT_TO_DIR, exist_ok=True)\n","\n","    # 3. Unpack the dataset using the 'unzip' command\n","    if not os.path.exists(os.path.join(EXTRACT_TO_DIR, \"0_real\")):\n","        print(f\"üöÄ Starting to unpack '{ZIP_FILE_PATH}'...\")\n","        # Use the 'unzip' command with -q for quiet mode and -d for destination\n","        !unzip -q \"{ZIP_FILE_PATH}\" -d \"{EXTRACT_TO_DIR}\"\n","        print(\"‚úÖ Unpacking complete!\")\n","    else:\n","        print(\"‚úÖ Dataset already unpacked.\")\n","except Exception as e:\n","    print(f\"‚ùå An error occurred during unpacking: {e}\")\n","    raise\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PhCV-5p5CXA2","executionInfo":{"status":"ok","timestamp":1759583818579,"user_tz":-360,"elapsed":252880,"user":{"displayName":"Affshafee Rahman","userId":"16838251676656766360"}},"outputId":"215d906c-1ae0-464b-dd0b-df5a3c224032"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","================================================================================\n","üìÇ STEP 1: UNPACKING DATASET FROM .ZIP FILE\n","================================================================================\n","Mounted at /content/drive\n","üöÄ Starting to unpack '/content/drive/MyDrive/CSE400 codes - 144/AVLips.zip'...\n","‚úÖ Unpacking complete!\n"]}]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IQ7gJMJUCIRq","executionInfo":{"status":"ok","timestamp":1759587402688,"user_tz":-360,"elapsed":3354443,"user":{"displayName":"Affshafee Rahman","userId":"16838251676656766360"}},"outputId":"db564751-beee-4a1d-899d-db26338369c5"},"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ Libraries imported successfully.\n","\n","================================================================================\n","üöÄ V1 MAMBA ABLATION STUDY - BATCH 2\n","Testing: V1e (192,192), V1f (128,96), V1g (96,128)\n","================================================================================\n","\n","\n","================================================================================\n","STEP 1: PREPARING FILE LISTS\n","================================================================================\n","üî• Sampling 2000 videos per class...\n","Total: 4000 | Train: 2800 | Val: 600 | Test: 600\n","\n","================================================================================\n","STEP 2: CACHING DATA (ONCE FOR ALL VARIANTS)\n","================================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Caching Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 44/44 [27:23<00:00, 37.35s/it]\n","Caching Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [05:54<00:00, 35.44s/it]\n","Caching Test: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [06:10<00:00, 37.05s/it]\n"]},{"output_type":"stream","name":"stdout","text":["‚úÖ Cached - Train: 1364, Val: 294, Test: 299\n","\n","================================================================================\n","üìä VARIANT 1/3: V1e\n","   Visual d_model=192, Audio d_model=192\n","   Started at: 14:00:29\n","================================================================================\n","\n","Downloading: \"https://download.pytorch.org/models/mobilenet_v3_small-047dcff4.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v3_small-047dcff4.pth\n"]},{"output_type":"stream","name":"stderr","text":["100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9.83M/9.83M [00:00<00:00, 124MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Model: 2,677,441 params (2.677M), 10.31 MB\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [05:03<00:00, 13.80s/it, loss=0.6901]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:05<00:00,  1.01s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/25 - Train: 0.6916, Val: 0.6918 ‚úÖ\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [00:08<00:00,  2.49it/s, loss=0.6295]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:01<00:00,  2.69it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2/25 - Train: 0.6758, Val: 0.6937\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [00:08<00:00,  2.51it/s, loss=0.5196]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:01<00:00,  2.74it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3/25 - Train: 0.6178, Val: 0.7836\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [00:08<00:00,  2.64it/s, loss=0.7709]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  2.33it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4/25 - Train: 0.5182, Val: 1.1365\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [00:08<00:00,  2.66it/s, loss=0.4635]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:01<00:00,  2.63it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5/25 - Train: 0.4045, Val: 0.9990\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [00:08<00:00,  2.53it/s, loss=0.2760]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:01<00:00,  2.59it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 6/25 - Train: 0.3316, Val: 0.8484\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [00:08<00:00,  2.49it/s, loss=0.2935]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:01<00:00,  2.62it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 7/25 - Train: 0.3060, Val: 0.8922\n","Early stopping at epoch 7\n"]},{"output_type":"stream","name":"stderr","text":["Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:51<00:00, 10.35s/it]\n"]},{"output_type":"stream","name":"stdout","text":["\n","‚úÖ V1e Results:\n","   Accuracy: 54.52%, AUC: 57.78%, Loss Gap: -0.5862\n","   Saved to: /content/v1_ablation_batch2_results.csv\n","\n","================================================================================\n","üìä VARIANT 2/3: V1f\n","   Visual d_model=128, Audio d_model=96\n","   Started at: 14:07:34\n","================================================================================\n","\n","Model: 2,225,761 params (2.226M), 8.58 MB\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [00:09<00:00,  2.36it/s, loss=0.6959]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:01<00:00,  2.59it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/25 - Train: 0.6933, Val: 0.6936 ‚úÖ\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [00:09<00:00,  2.41it/s, loss=0.6822]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:01<00:00,  2.50it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2/25 - Train: 0.6601, Val: 0.6815 ‚úÖ\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [00:08<00:00,  2.45it/s, loss=0.6065]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:01<00:00,  2.57it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3/25 - Train: 0.6136, Val: 0.6637 ‚úÖ\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [00:08<00:00,  2.46it/s, loss=0.5933]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:01<00:00,  2.63it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4/25 - Train: 0.5648, Val: 0.6406 ‚úÖ\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [00:08<00:00,  2.47it/s, loss=0.6485]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:01<00:00,  2.52it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5/25 - Train: 0.5218, Val: 0.6339 ‚úÖ\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [00:08<00:00,  2.55it/s, loss=0.4756]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:01<00:00,  2.67it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 6/25 - Train: 0.4878, Val: 0.7657\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [00:08<00:00,  2.48it/s, loss=0.4475]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:01<00:00,  2.60it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 7/25 - Train: 0.4501, Val: 0.7015\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [00:09<00:00,  2.41it/s, loss=0.3681]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:01<00:00,  2.66it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 8/25 - Train: 0.3948, Val: 0.7156\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [00:08<00:00,  2.51it/s, loss=0.3345]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:01<00:00,  2.60it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 9/25 - Train: 0.3177, Val: 0.8186\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [00:08<00:00,  2.62it/s, loss=0.2793]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  2.46it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 10/25 - Train: 0.2689, Val: 0.6737\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [00:08<00:00,  2.54it/s, loss=0.2633]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:01<00:00,  2.56it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 11/25 - Train: 0.2588, Val: 0.6164 ‚úÖ\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [00:08<00:00,  2.48it/s, loss=0.2755]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:01<00:00,  2.55it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 12/25 - Train: 0.2509, Val: 0.6520\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [00:08<00:00,  2.51it/s, loss=0.2389]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:01<00:00,  2.61it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 13/25 - Train: 0.2406, Val: 0.5007 ‚úÖ\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [00:08<00:00,  2.47it/s, loss=0.2326]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:01<00:00,  2.61it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 14/25 - Train: 0.2318, Val: 0.4875 ‚úÖ\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [00:08<00:00,  2.54it/s, loss=0.2308]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  2.46it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 15/25 - Train: 0.2272, Val: 0.4652 ‚úÖ\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [00:08<00:00,  2.50it/s, loss=0.2239]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  2.47it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 16/25 - Train: 0.2261, Val: 0.4361 ‚úÖ\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [00:08<00:00,  2.51it/s, loss=0.2326]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  2.48it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 17/25 - Train: 0.2229, Val: 0.4224 ‚úÖ\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [00:08<00:00,  2.53it/s, loss=0.2216]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:01<00:00,  2.57it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 18/25 - Train: 0.2212, Val: 0.4475\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [00:09<00:00,  2.38it/s, loss=0.2146]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:01<00:00,  2.56it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 19/25 - Train: 0.2197, Val: 0.4191 ‚úÖ\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [00:09<00:00,  2.43it/s, loss=0.2100]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:01<00:00,  2.54it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 20/25 - Train: 0.2180, Val: 0.4323\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [00:08<00:00,  2.48it/s, loss=0.2248]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:01<00:00,  2.60it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 21/25 - Train: 0.2188, Val: 0.3976 ‚úÖ\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [00:08<00:00,  2.66it/s, loss=0.2280]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:01<00:00,  2.56it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 22/25 - Train: 0.2160, Val: 0.3934 ‚úÖ\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [00:08<00:00,  2.53it/s, loss=0.2308]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:01<00:00,  2.66it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 23/25 - Train: 0.2163, Val: 0.4134\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [00:08<00:00,  2.55it/s, loss=0.2305]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  2.49it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 24/25 - Train: 0.2149, Val: 0.3930 ‚úÖ\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [00:08<00:00,  2.50it/s, loss=0.2419]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:01<00:00,  2.59it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 25/25 - Train: 0.2151, Val: 0.3725 ‚úÖ\n"]},{"output_type":"stream","name":"stderr","text":["Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  2.43it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","‚úÖ V1f Results:\n","   Accuracy: 88.96%, AUC: 96.23%, Loss Gap: -0.1574\n","   Saved to: /content/v1_ablation_batch2_results.csv\n","\n","================================================================================\n","üìä VARIANT 3/3: V1g\n","   Visual d_model=96, Audio d_model=128\n","   Started at: 14:12:07\n","================================================================================\n","\n","Model: 2,225,761 params (2.226M), 8.58 MB\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [00:09<00:00,  2.42it/s, loss=0.6998]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:01<00:00,  2.64it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/25 - Train: 0.6924, Val: 0.6925 ‚úÖ\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [00:08<00:00,  2.52it/s, loss=0.6340]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:01<00:00,  2.59it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2/25 - Train: 0.6780, Val: 0.6869 ‚úÖ\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [00:08<00:00,  2.58it/s, loss=0.5042]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  2.44it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3/25 - Train: 0.6201, Val: 0.6827 ‚úÖ\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [00:08<00:00,  2.50it/s, loss=0.5511]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:01<00:00,  2.61it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4/25 - Train: 0.5632, Val: 0.6655 ‚úÖ\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [00:09<00:00,  2.43it/s, loss=0.5267]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:01<00:00,  2.55it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5/25 - Train: 0.5242, Val: 0.6479 ‚úÖ\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [00:08<00:00,  2.46it/s, loss=0.4730]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:01<00:00,  2.65it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 6/25 - Train: 0.4777, Val: 0.6407 ‚úÖ\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [00:09<00:00,  2.39it/s, loss=0.3731]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:01<00:00,  2.58it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 7/25 - Train: 0.4193, Val: 0.5507 ‚úÖ\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [00:08<00:00,  2.54it/s, loss=0.4225]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:01<00:00,  2.54it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 8/25 - Train: 0.3414, Val: 0.5614\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [00:08<00:00,  2.46it/s, loss=0.2377]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:01<00:00,  2.56it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 9/25 - Train: 0.2761, Val: 0.5437 ‚úÖ\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [00:08<00:00,  2.53it/s, loss=0.2384]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:01<00:00,  2.62it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 10/25 - Train: 0.2629, Val: 0.4864 ‚úÖ\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [00:08<00:00,  2.55it/s, loss=0.2307]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:01<00:00,  2.54it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 11/25 - Train: 0.2452, Val: 0.5040\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [00:09<00:00,  2.42it/s, loss=0.2378]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:01<00:00,  2.60it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 12/25 - Train: 0.2399, Val: 0.4734 ‚úÖ\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [00:09<00:00,  2.40it/s, loss=0.2280]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:01<00:00,  2.56it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 13/25 - Train: 0.2306, Val: 0.3658 ‚úÖ\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [00:08<00:00,  2.45it/s, loss=0.2235]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:01<00:00,  2.65it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 14/25 - Train: 0.2297, Val: 0.3237 ‚úÖ\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [00:09<00:00,  2.39it/s, loss=0.2133]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  2.46it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 15/25 - Train: 0.2218, Val: 0.3187 ‚úÖ\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [00:08<00:00,  2.49it/s, loss=0.2186]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:01<00:00,  2.60it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 16/25 - Train: 0.2202, Val: 0.3003 ‚úÖ\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [00:08<00:00,  2.53it/s, loss=0.2202]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  2.49it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 17/25 - Train: 0.2191, Val: 0.3367\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [00:09<00:00,  2.42it/s, loss=0.2131]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  2.49it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 18/25 - Train: 0.2143, Val: 0.3394\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [00:09<00:00,  2.42it/s, loss=0.2247]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:01<00:00,  2.61it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 19/25 - Train: 0.2153, Val: 0.3301\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [00:09<00:00,  2.41it/s, loss=0.2393]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:01<00:00,  2.67it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 20/25 - Train: 0.2146, Val: 0.3294\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [00:08<00:00,  2.50it/s, loss=0.2088]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:01<00:00,  2.61it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 21/25 - Train: 0.2133, Val: 0.2969 ‚úÖ\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [00:08<00:00,  2.45it/s, loss=0.2087]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  2.43it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 22/25 - Train: 0.2119, Val: 0.3121\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [00:08<00:00,  2.46it/s, loss=0.2538]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  2.48it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 23/25 - Train: 0.2135, Val: 0.3641\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [00:08<00:00,  2.50it/s, loss=0.2201]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:01<00:00,  2.64it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 24/25 - Train: 0.2107, Val: 0.3334\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [00:09<00:00,  2.44it/s, loss=0.2082]\n","Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  2.47it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 25/25 - Train: 0.2103, Val: 0.3342\n"]},{"output_type":"stream","name":"stderr","text":["Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  2.40it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","‚úÖ V1g Results:\n","   Accuracy: 92.98%, AUC: 97.72%, Loss Gap: -0.1239\n","   Saved to: /content/v1_ablation_batch2_results.csv\n","\n","================================================================================\n","üéâ BATCH 2 COMPLETED!\n","================================================================================\n","Total time: 0.93 hours\n","\n","Results Summary:\n","  variant  params_M  accuracy       auc  loss_gap\n","0     V1e  2.677441  0.545151  0.577792 -0.586154\n","1     V1f  2.225761  0.889632  0.962333 -0.157431\n","2     V1g  2.225761  0.929766  0.977174 -0.123884\n","\n","üìÅ Results saved to: /content/v1_ablation_batch2_results.csv\n"]}],"source":["# -*- coding: utf-8 -*-\n","\n","\"\"\"\n","V1 MAMBA ABLATION STUDY - BATCH 2\n","Testing V1e (192,192), V1f (128,96), V1g (96,128)\n","\"\"\"\n","\n","# --- 1. IMPORTS ---\n","import os\n","import cv2\n","import time\n","import torch\n","import librosa\n","import numpy as np\n","import pandas as pd\n","import torch.nn as nn\n","from pathlib import Path\n","from tqdm import tqdm\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","import matplotlib.pyplot as plt\n","import torch.nn.functional as F\n","import torchvision.models as models\n","from mamba_ssm import Mamba\n","from torch.cuda.amp import autocast, GradScaler\n","import torchvision.transforms as transforms\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report, roc_auc_score\n","import warnings\n","\n","warnings.filterwarnings('ignore')\n","torch.backends.cudnn.benchmark = True\n","print(\"‚úÖ Libraries imported successfully.\")\n","\n","# --- 2. CONFIGURATION ---\n","class Config:\n","    def __init__(self, vis_d_model=128, aud_d_model=128):\n","        self.data_dir = \"/content/AVLips_data/AVLips\"\n","        self.model_save_dir = \"/content/models/\"\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        os.makedirs(self.model_save_dir, exist_ok=True)\n","\n","        self.use_sampling = True\n","        self.num_samples_per_class = 2000\n","\n","        # Visual Stream\n","        self.vis_image_size = (128, 128)\n","        self.vis_num_frames = 16\n","        self.vis_cnn_feature_dim = 576\n","        self.vis_mamba_d_model = vis_d_model  # CONFIGURABLE\n","\n","        # Audio Stream\n","        self.aud_sample_rate = 16000\n","        self.aud_num_chunks = 5\n","        self.aud_chunk_duration = 1.0\n","        self.aud_n_mels = 128\n","        self.aud_cnn_feature_dim = 576\n","        self.aud_mamba_d_model = aud_d_model  # CONFIGURABLE\n","\n","        # Training\n","        self.batch_size = 64\n","        self.accumulation_steps = 4\n","        self.epochs = 25\n","        self.learning_rate = 5e-4\n","        self.weight_decay = 0.05\n","        self.patience = 6\n","\n","# --- 3. LABEL SMOOTHING LOSS ---\n","class LabelSmoothingBCELoss(nn.Module):\n","    def __init__(self, smoothing=0.1):\n","        super().__init__()\n","        self.smoothing = smoothing\n","\n","    def forward(self, pred, target):\n","        target = target * (1 - self.smoothing) + 0.5 * self.smoothing\n","        return F.binary_cross_entropy_with_logits(pred, target)\n","\n","# --- 4. DATA PROCESSING ---\n","def process_visual_stream(video_path: str, config: Config):\n","    cap = cv2.VideoCapture(video_path)\n","    if not cap.isOpened():\n","        return None\n","\n","    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","    if total_frames < config.vis_num_frames:\n","        return None\n","\n","    frame_indices = np.linspace(0, total_frames - 1, config.vis_num_frames, dtype=int)\n","    frames = []\n","    face_detector = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n","\n","    for i in frame_indices:\n","        cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n","        ret, frame = cap.read()\n","        if not ret:\n","            continue\n","\n","        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n","        faces = face_detector.detectMultiScale(gray, 1.1, 4)\n","\n","        if len(faces) > 0:\n","            (x, y, w, h) = faces[0]\n","            mouth_crop = frame[y + int(h * 0.6):y + h, x + int(w * 0.25):x + int(w * 0.75)]\n","            if mouth_crop.size > 0:\n","                resized_crop = cv2.resize(mouth_crop, config.vis_image_size)\n","                resized_crop_rgb = cv2.cvtColor(resized_crop, cv2.COLOR_BGR2RGB)\n","                frames.append(resized_crop_rgb)\n","\n","    cap.release()\n","    return np.stack(frames) if len(frames) == config.vis_num_frames else None\n","\n","def process_audio_stream(video_path: str, config: Config):\n","    try:\n","        parts = Path(video_path).parts\n","        audio_filename = Path(video_path).stem + \".wav\"\n","        label_folder = parts[-2]\n","        base_data_dir = str(Path(video_path).parent.parent)\n","        audio_path = os.path.join(base_data_dir, \"wav\", label_folder, audio_filename)\n","\n","        y, sr = librosa.load(audio_path, sr=config.aud_sample_rate)\n","        total_samples = int(config.aud_chunk_duration * config.aud_num_chunks * sr)\n","\n","        if len(y) < total_samples:\n","            y = np.pad(y, (0, total_samples - len(y)), mode='constant')\n","        else:\n","            y = y[:total_samples]\n","\n","        samples_per_chunk = int(config.aud_chunk_duration * sr)\n","        mel_list = []\n","\n","        for i in range(config.aud_num_chunks):\n","            chunk = y[i*samples_per_chunk : (i+1)*samples_per_chunk]\n","            mel = librosa.feature.melspectrogram(y=chunk, sr=sr, n_mels=config.aud_n_mels)\n","            mel_db = librosa.power_to_db(mel, ref=np.max)\n","            mel_db = (mel_db - mel_db.mean()) / (mel_db.std() + 1e-9)\n","            mel_list.append(torch.tensor(mel_db, dtype=torch.float32))\n","\n","        return torch.stack(mel_list, axis=0)\n","    except Exception:\n","        return None\n","\n","class DualStreamDataset(Dataset):\n","    def __init__(self, file_paths, labels, config):\n","        self.file_paths = file_paths\n","        self.labels = labels\n","        self.config = config\n","\n","    def __len__(self):\n","        return len(self.file_paths)\n","\n","    def __getitem__(self, idx):\n","        video_path = self.file_paths[idx]\n","        label = self.labels[idx]\n","\n","        try:\n","            visual_frames_hwc = process_visual_stream(video_path, self.config)\n","            if visual_frames_hwc is None:\n","                return None\n","\n","            visual_frames_tchw = visual_frames_hwc.transpose(0, 3, 1, 2)\n","            audio_mels = process_audio_stream(video_path, self.config)\n","            if audio_mels is None:\n","                return None\n","\n","            audio_tensor = audio_mels.unsqueeze(1)\n","            return (visual_frames_tchw, audio_tensor), torch.tensor(label, dtype=torch.float32)\n","        except Exception:\n","            return None\n","\n","class RAMCachedDataset(Dataset):\n","    def __init__(self, data, labels, transform=None):\n","        self.data = data\n","        self.labels = labels\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def __getitem__(self, idx):\n","        visual_frames_np, audio_tensor = self.data[idx]\n","        label = self.labels[idx]\n","\n","        if self.transform:\n","            augmented_frames = []\n","            for frame_np in visual_frames_np:\n","                frame_hwc = frame_np.transpose(1, 2, 0)\n","                augmented_frames.append(self.transform(frame_hwc))\n","            visual_tensor = torch.stack(augmented_frames)\n","        else:\n","            visual_tensor = torch.from_numpy(visual_frames_np).float()\n","\n","        return (visual_tensor, audio_tensor), label\n","\n","# --- 5. MODEL ARCHITECTURE ---\n","class VisualStream_MobileNetV3Small(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        mobilenet = models.mobilenet_v3_small(weights=models.MobileNet_V3_Small_Weights.DEFAULT)\n","        self.cnn_features = mobilenet.features\n","        self.avgpool = mobilenet.avgpool\n","\n","        self.proj = nn.Linear(config.vis_cnn_feature_dim, config.vis_mamba_d_model)\n","        self.proj_dropout = nn.Dropout(0.3)\n","        self.mamba = Mamba(d_model=config.vis_mamba_d_model, d_state=16, d_conv=4, expand=2)\n","        self.mamba_dropout = nn.Dropout(0.2)\n","\n","    def forward(self, x):\n","        b, t, c, h, w = x.shape\n","        x = x.view(b * t, c, h, w)\n","\n","        features = self.cnn_features(x)\n","        features = self.avgpool(features)\n","        features = features.view(b, t, -1)\n","\n","        projected_features = self.proj_dropout(self.proj(features))\n","        temporal_out = self.mamba(projected_features)\n","        temporal_out = self.mamba_dropout(temporal_out)\n","\n","        return temporal_out[:, -1, :]\n","\n","class AudioStream_MobileNetV3Small(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        mobilenet = models.mobilenet_v3_small(weights=models.MobileNet_V3_Small_Weights.DEFAULT)\n","        self.cnn_features = mobilenet.features\n","        self.avgpool = mobilenet.avgpool\n","\n","        self.proj = nn.Linear(config.aud_cnn_feature_dim, config.aud_mamba_d_model)\n","        self.proj_dropout = nn.Dropout(0.3)\n","        self.mamba = Mamba(d_model=config.aud_mamba_d_model, d_state=16, d_conv=4, expand=2)\n","        self.mamba_dropout = nn.Dropout(0.2)\n","\n","    def forward(self, x):\n","        b, t, c, h, w = x.shape\n","        x = x.view(b * t, c, h, w).repeat(1, 3, 1, 1)\n","\n","        features = self.cnn_features(x)\n","        features = self.avgpool(features)\n","        features = features.view(b, t, -1)\n","\n","        projected_features = self.proj_dropout(self.proj(features))\n","        temporal_out = self.mamba(projected_features)\n","        temporal_out = self.mamba_dropout(temporal_out)\n","\n","        return temporal_out[:, -1, :]\n","\n","class FusionModel_V1(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.visual_stream = VisualStream_MobileNetV3Small(config)\n","        self.audio_stream = AudioStream_MobileNetV3Small(config)\n","\n","        fusion_input_dim = config.vis_mamba_d_model + config.aud_mamba_d_model\n","        self.fusion_head = nn.Sequential(\n","            nn.Linear(fusion_input_dim, 256),\n","            nn.ReLU(),\n","            nn.Dropout(0.6),\n","            nn.Linear(256, 1)\n","        )\n","\n","    def forward(self, visual_input, audio_input):\n","        visual_features = self.visual_stream(visual_input)\n","        audio_features = self.audio_stream(audio_input)\n","        fused_features = torch.cat((visual_features, audio_features), dim=1)\n","        return self.fusion_head(fused_features)\n","\n","# --- 6. UTILITY FUNCTIONS ---\n","def count_parameters(model):\n","    total = sum(p.numel() for p in model.parameters())\n","    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","    return total, trainable\n","\n","def get_model_size_mb(model):\n","    param_size = sum(p.nelement() * p.element_size() for p in model.parameters())\n","    buffer_size = sum(b.nelement() * b.element_size() for b in model.buffers())\n","    return (param_size + buffer_size) / (1024 ** 2)\n","\n","# --- 7. TRAINING FUNCTIONS ---\n","def train_one_epoch(model, loader, optimizer, criterion, scaler, config):\n","    model.train()\n","    total_loss = 0\n","    pbar = tqdm(loader, desc=\"Training\")\n","\n","    for i, ((visual_data, audio_data), labels) in enumerate(pbar):\n","        visual_data = visual_data.to(config.device, non_blocking=True)\n","        audio_data = audio_data.to(config.device, non_blocking=True)\n","        labels = labels.to(config.device, non_blocking=True).unsqueeze(1).float()\n","\n","        with autocast():\n","            outputs = model(visual_data, audio_data)\n","            loss = criterion(outputs, labels)\n","            loss = loss / config.accumulation_steps\n","\n","        scaler.scale(loss).backward()\n","\n","        if (i + 1) % config.accumulation_steps == 0:\n","            scaler.unscale_(optimizer)\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","            scaler.step(optimizer)\n","            scaler.update()\n","            optimizer.zero_grad(set_to_none=True)\n","\n","        total_loss += loss.item() * config.accumulation_steps\n","        pbar.set_postfix({'loss': f\"{loss.item() * config.accumulation_steps:.4f}\"})\n","\n","    return total_loss / len(loader)\n","\n","def validate_one_epoch(model, loader, criterion, config):\n","    model.eval()\n","    total_loss = 0\n","\n","    with torch.no_grad():\n","        for (visual_data, audio_data), labels in tqdm(loader, desc=\"Validating\"):\n","            visual_data = visual_data.to(config.device, non_blocking=True)\n","            audio_data = audio_data.to(config.device, non_blocking=True)\n","            labels = labels.to(config.device, non_blocking=True).unsqueeze(1).float()\n","\n","            with autocast():\n","                outputs = model(visual_data, audio_data)\n","                loss = criterion(outputs, labels)\n","\n","            total_loss += loss.item()\n","\n","    return total_loss / len(loader)\n","\n","def evaluate_model(model, test_loader, config):\n","    model.eval()\n","    all_labels, all_preds = [], []\n","\n","    with torch.no_grad():\n","        for (visual_data, audio_data), labels in tqdm(test_loader, desc=\"Evaluating\"):\n","            visual_data = visual_data.to(config.device)\n","            audio_data = audio_data.to(config.device)\n","\n","            outputs = model(visual_data, audio_data)\n","            all_preds.extend(torch.sigmoid(outputs).cpu().numpy())\n","            all_labels.extend(labels.numpy())\n","\n","    all_preds = np.array(all_preds).flatten()\n","    all_labels = np.array(all_labels).flatten()\n","    preds_binary = (all_preds > 0.5).astype(int)\n","\n","    accuracy = (preds_binary == all_labels).mean()\n","    auc_score = roc_auc_score(all_labels, all_preds)\n","\n","    return accuracy, auc_score, all_labels, preds_binary\n","\n","# --- 8. MAIN EXECUTION ---\n","def main():\n","    print(\"\\n\" + \"=\"*80)\n","    print(\"üöÄ V1 MAMBA ABLATION STUDY - BATCH 2\")\n","    print(\"Testing: V1e (192,192), V1f (128,96), V1g (96,128)\")\n","    print(\"=\"*80 + \"\\n\")\n","\n","    start_time = time.time()\n","\n","    # Initialize base config for data caching\n","    base_config = Config()\n","\n","    print(\"\\n\" + \"=\"*80 + \"\\nSTEP 1: PREPARING FILE LISTS\\n\" + \"=\"*80)\n","    real_dir = os.path.join(base_config.data_dir, \"0_real\")\n","    fake_dir = os.path.join(base_config.data_dir, \"1_fake\")\n","\n","    all_real = [os.path.join(real_dir, f) for f in os.listdir(real_dir) if f.endswith('.mp4')]\n","    all_fake = [os.path.join(fake_dir, f) for f in os.listdir(fake_dir) if f.endswith('.mp4')]\n","\n","    if base_config.use_sampling:\n","        print(f\"üî• Sampling {base_config.num_samples_per_class} videos per class...\")\n","        real_files = np.random.choice(all_real, base_config.num_samples_per_class, replace=False).tolist()\n","        fake_files = np.random.choice(all_fake, base_config.num_samples_per_class, replace=False).tolist()\n","    else:\n","        real_files, fake_files = all_real, all_fake\n","\n","    all_files = real_files + fake_files\n","    labels = [0] * len(real_files) + [1] * len(fake_files)\n","\n","    train_files, test_files, train_labels, test_labels = train_test_split(\n","        all_files, labels, test_size=0.3, random_state=42, stratify=labels)\n","    val_files, test_files, val_labels, test_labels = train_test_split(\n","        test_files, test_labels, test_size=0.5, random_state=42, stratify=test_labels)\n","\n","    print(f\"Total: {len(all_files)} | Train: {len(train_files)} | Val: {len(val_files)} | Test: {len(test_files)}\")\n","\n","    print(\"\\n\" + \"=\"*80 + \"\\nSTEP 2: CACHING DATA (ONCE FOR ALL VARIANTS)\\n\" + \"=\"*80)\n","\n","    def collate_fn_skip_errors(batch):\n","        batch = list(filter(lambda x: x is not None, batch))\n","        return torch.utils.data.dataloader.default_collate(batch) if batch else (None, None)\n","\n","    def cache_data(files, labels, desc):\n","        dataset = DualStreamDataset(files, labels, base_config)\n","        loader = DataLoader(dataset, batch_size=base_config.batch_size, num_workers=os.cpu_count(), collate_fn=collate_fn_skip_errors)\n","        cached_data, cached_labels = [], []\n","\n","        for data, batch_labels in tqdm(loader, desc=f\"Caching {desc}\"):\n","            if data is not None:\n","                visual_batch, audio_batch = data\n","                for i in range(visual_batch.shape[0]):\n","                    cached_data.append((visual_batch[i].numpy(), audio_batch[i]))\n","                    cached_labels.append(batch_labels[i])\n","\n","        return cached_data, torch.tensor(cached_labels)\n","\n","    cached_train_data, cached_train_labels = cache_data(train_files, train_labels, \"Train\")\n","    cached_val_data, cached_val_labels = cache_data(val_files, val_labels, \"Val\")\n","    cached_test_data, cached_test_labels = cache_data(test_files, test_labels, \"Test\")\n","\n","    print(f\"‚úÖ Cached - Train: {len(cached_train_data)}, Val: {len(cached_val_data)}, Test: {len(cached_test_data)}\")\n","\n","    # Define transforms\n","    val_test_transform = transforms.Compose([\n","        transforms.ToPILImage(),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","    ])\n","\n","    train_transform = transforms.Compose([\n","        transforms.ToPILImage(),\n","        transforms.RandomHorizontalFlip(p=0.5),\n","        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n","        transforms.RandomRotation(10),\n","        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","    ])\n","\n","    # Variants to test (BATCH 2)\n","    configs_to_test = [\n","        (192, 192, 'V1e'),\n","        (128, 96, 'V1f'),\n","        (96, 128, 'V1g'),\n","    ]\n","\n","    results = []\n","\n","    for idx, (vis_d, aud_d, variant_name) in enumerate(configs_to_test):\n","        print(\"\\n\" + \"=\"*80)\n","        print(f\"üìä VARIANT {idx+1}/3: {variant_name}\")\n","        print(f\"   Visual d_model={vis_d}, Audio d_model={aud_d}\")\n","        print(f\"   Started at: {time.strftime('%H:%M:%S')}\")\n","        print(\"=\"*80 + \"\\n\")\n","\n","        # Create config for this variant\n","        config = Config(vis_d_model=vis_d, aud_d_model=aud_d)\n","\n","        # Create dataloaders (reusing cached data!)\n","        train_dataset = RAMCachedDataset(cached_train_data, cached_train_labels, transform=train_transform)\n","        val_dataset = RAMCachedDataset(cached_val_data, cached_val_labels, transform=val_test_transform)\n","        test_dataset = RAMCachedDataset(cached_test_data, cached_test_labels, transform=val_test_transform)\n","\n","        train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=os.cpu_count(), pin_memory=True)\n","        val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False, num_workers=os.cpu_count(), pin_memory=True)\n","        test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False, num_workers=os.cpu_count(), pin_memory=True)\n","\n","        # Build model\n","        model = FusionModel_V1(config).to(config.device)\n","        total_params, trainable_params = count_parameters(model)\n","        model_size_mb = get_model_size_mb(model)\n","\n","        print(f\"Model: {total_params:,} params ({total_params/1e6:.3f}M), {model_size_mb:.2f} MB\")\n","\n","        # Training setup\n","        optimizer = optim.AdamW(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n","        criterion = LabelSmoothingBCELoss(smoothing=0.1)\n","        scaler = GradScaler()\n","        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, min_lr=1e-6)\n","\n","        model_path = os.path.join(config.model_save_dir, f'{variant_name.lower()}_best.pth')\n","        best_val_loss = float('inf')\n","        epochs_no_improve = 0\n","        history = {'train_loss': [], 'val_loss': []}\n","\n","        # Training loop\n","        for epoch in range(config.epochs):\n","            train_loss = train_one_epoch(model, train_loader, optimizer, criterion, scaler, config)\n","            val_loss = validate_one_epoch(model, val_loader, criterion, config)\n","\n","            history['train_loss'].append(train_loss)\n","            history['val_loss'].append(val_loss)\n","\n","            scheduler.step(val_loss)\n","\n","            if val_loss < best_val_loss:\n","                best_val_loss = val_loss\n","                epochs_no_improve = 0\n","                torch.save(model.state_dict(), model_path)\n","                print(f\"Epoch {epoch+1}/{config.epochs} - Train: {train_loss:.4f}, Val: {val_loss:.4f} ‚úÖ\")\n","            else:\n","                epochs_no_improve += 1\n","                print(f\"Epoch {epoch+1}/{config.epochs} - Train: {train_loss:.4f}, Val: {val_loss:.4f}\")\n","                if epochs_no_improve >= config.patience:\n","                    print(f\"Early stopping at epoch {epoch+1}\")\n","                    break\n","\n","        # Final evaluation\n","        model.load_state_dict(torch.load(model_path))\n","        accuracy, auc_score, all_labels, preds_binary = evaluate_model(model, test_loader, config)\n","        loss_gap = history['train_loss'][-1] - history['val_loss'][-1]\n","\n","        print(f\"\\n‚úÖ {variant_name} Results:\")\n","        print(f\"   Accuracy: {accuracy*100:.2f}%, AUC: {auc_score*100:.2f}%, Loss Gap: {loss_gap:.4f}\")\n","\n","        # Save results\n","        results.append({\n","            'variant': variant_name,\n","            'vis_d_model': vis_d,\n","            'aud_d_model': aud_d,\n","            'params_M': total_params / 1e6,\n","            'size_MB': model_size_mb,\n","            'accuracy': accuracy,\n","            'auc': auc_score,\n","            'loss_gap': loss_gap,\n","            'completed_at': time.strftime('%Y-%m-%d %H:%M:%S')\n","        })\n","\n","        # Save intermediate results\n","        df = pd.DataFrame(results)\n","        df.to_csv('/content/v1_ablation_batch2_results.csv', index=False)\n","        print(f\"   Saved to: /content/v1_ablation_batch2_results.csv\")\n","\n","        # Cleanup\n","        del model\n","        torch.cuda.empty_cache()\n","\n","    # Final summary\n","    total_time = (time.time() - start_time) / 3600\n","    print(\"\\n\" + \"=\"*80)\n","    print(\"üéâ BATCH 2 COMPLETED!\")\n","    print(\"=\"*80)\n","    print(f\"Total time: {total_time:.2f} hours\")\n","    print(\"\\nResults Summary:\")\n","    print(pd.DataFrame(results)[['variant', 'params_M', 'accuracy', 'auc', 'loss_gap']])\n","    print(\"\\nüìÅ Results saved to: /content/v1_ablation_batch2_results.csv\")\n","\n","if __name__ == '__main__':\n","    main()\n"]}]}